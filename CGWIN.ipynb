{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paper Information :\n",
    "    Generative Well-intentioned Networks \n",
    "    https://papers.nips.cc/paper/9467-generative-well-intentioned-networks.pdf\n",
    "\n",
    "Authors of code:\n",
    "    Yasin Berk GÃ¼ltekin - 1942119\n",
    "    Hasan Ali Duran - 1942119\n",
    "    \n",
    "    \n",
    "Challenges Encountered When Implementing Paper:\n",
    "* There was not enough detail about how BNN was implemented. The number of layers was not specified. We could not fully obtain the BNN results mentioned in Paper. The BNN we have implemented makes predictions with higher scores. This situation caused difficulties in the exact occurrence of qualitative results.\n",
    "\n",
    "* The biggest problem we encountered during GAN implementation was that it was not clear enough how the Generator and Discriminator inputs should be processed in the model. It was not specified how many layers or what types of layers were used. In addition, the pictures produced by the generator appeared similar to those given as input to the Generator. There was no explanation for how this problem was solved in paper implementation. The new method which is transformation loss used during the Discriminator's loss calculation was not sufficiently explained.(You can find our assumptions for the models(like number of layers and type of layers) in the implementation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-user install because site-packages writeable\n",
      "Created temporary directory: C:\\Users\\Lenovo\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-x0ausrdi\n",
      "Created temporary directory: C:\\Users\\Lenovo\\AppData\\Local\\Temp\\pip-req-tracker-ntczw9ee\n",
      "Initialized build tracking at C:\\Users\\Lenovo\\AppData\\Local\\Temp\\pip-req-tracker-ntczw9ee\n",
      "Created build tracker: C:\\Users\\Lenovo\\AppData\\Local\\Temp\\pip-req-tracker-ntczw9ee\n",
      "Entered build tracker: C:\\Users\\Lenovo\\AppData\\Local\\Temp\\pip-req-tracker-ntczw9ee\n",
      "Created temporary directory: C:\\Users\\Lenovo\\AppData\\Local\\Temp\\pip-install-vmsntved\n",
      "Requirement already satisfied: pyro-ppl in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (1.3.1)\n",
      "Requirement already satisfied: pyro-api>=0.1.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from pyro-ppl) (0.1.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from pyro-ppl) (3.2.1)\n",
      "Requirement already satisfied: tqdm>=4.36 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from pyro-ppl) (4.42.1)\n",
      "Requirement already satisfied: torch>=1.4.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from pyro-ppl) (1.5.0)\n",
      "Requirement already satisfied: numpy>=1.7 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from pyro-ppl) (1.18.1)\n",
      "Requirement already satisfied: future in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from torch>=1.4.0->pyro-ppl) (0.18.2)\n",
      "Cleaning up...\n",
      "Removed build tracker: 'C:\\\\Users\\\\Lenovo\\\\AppData\\\\Local\\\\Temp\\\\pip-req-tracker-ntczw9ee'\n"
     ]
    }
   ],
   "source": [
    "!pip install -v pyro-ppl\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets\n",
    "import os.path\n",
    "import os\n",
    "import numpy as np\n",
    "from torchvision.utils import save_image\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.autograd as autograd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import subprocess\n",
    "import pyro.distributions as dist\n",
    "import pyro\n",
    "from matplotlib import colors\n",
    "from pyro import optim\n",
    "from pyro.infer import SVI, Trace_ELBO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HyperParameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    \"threshold\": [0.7, 0.8, 0.9],\n",
    "    \"critic_threshold\": 0.95,\n",
    "    \"n_critic\": 5,\n",
    "    \"n_epochs_Classifier\": 30,\n",
    "    \"n_epochs_GAN\": 200000,\n",
    "    \"batch_size\": 128,\n",
    "    \"lr_Classifier\": 0.001, # learning rate for Classifier model\n",
    "    \"lr_GAN\": 0.0001,  # learning rate for GAN models\n",
    "    \"b1\": 0.5,\n",
    "    \"b2\": 0.9,\n",
    "    \"latent_dim\": 100,\n",
    "    \"n_classes\": 10,\n",
    "    \"img_size\": 28,\n",
    "    \"channels\": 1,\n",
    "    \"lambda_gp\": 10, # lambda for gradient penalty\n",
    "    \"lambda_loss\": 10, # lambda for transformation penalty\n",
    "    \"continue_on_existing_training\": 0,\n",
    "    \"cuda\": 1,\n",
    "    \"run_download_sh\": 0\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_shape = (parameters[\"channels\"], parameters[\"img_size\"], parameters[\"img_size\"])\n",
    "\n",
    "cuda = True if parameters[\"cuda\"] else False\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "FloatTensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if cuda else torch.LongTensor\n",
    "\n",
    "# ---------------------\n",
    "#  Setting Datasets\n",
    "# ---------------------\n",
    "if parameters[\"run_download_sh\"]:\n",
    "    subprocess.call(\"download.sh\", shell=True)\n",
    "\n",
    "trainset = datasets.MNIST(\n",
    "    root=\"\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.Compose(\n",
    "        [transforms.Resize(parameters[\"img_size\"]), transforms.ToTensor(), transforms.Normalize([0.5], [0.5])]))\n",
    "\n",
    "testset = datasets.MNIST(\n",
    "    root=\"\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transforms.Compose(\n",
    "        [transforms.Resize(parameters[\"img_size\"]), transforms.ToTensor(), transforms.Normalize([0.5], [0.5])]))\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset,\n",
    "    drop_last=True,\n",
    "    batch_size=parameters[\"batch_size\"],\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset,\n",
    "    drop_last=True,\n",
    "    batch_size=parameters[\"batch_size\"],\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classifier Model (Bayesian Neural Network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------\n",
    "#  BNN - Classifier Model\n",
    "# ---------------------\n",
    "class BNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BNN, self).__init__()\n",
    "\n",
    "        self.out = nn.Sequential(\n",
    "            nn.Linear(parameters[\"img_size\"] ** 2, 1024),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(1024, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.flatten(1)\n",
    "        return self.out(x)\n",
    "\n",
    "\n",
    "model = BNN()\n",
    "if cuda:\n",
    "    model.cuda()\n",
    "\n",
    "\n",
    "# ---------------------\n",
    "#  Stochastic Variational Inference's Module\n",
    "# ---------------------\n",
    "def module(x, y):\n",
    "    priors = {}\n",
    "    for iterator in model.named_parameters():\n",
    "        name, param = iterator\n",
    "        zeros = torch.zeros_like(param.data)\n",
    "        ones = torch.ones_like(param.data)\n",
    "        priors[name] = dist.Normal(loc=zeros,\n",
    "                                   scale=ones)\n",
    "\n",
    "    lifted_module = pyro.random_module(\"module\", model, priors)\n",
    "    lifted_module_method = lifted_module()\n",
    "    lhat = F.log_softmax(lifted_module_method(x), 1)\n",
    "    pyro.sample(\"obs\", dist.Categorical(logits=lhat), obs=y)\n",
    "\n",
    "\n",
    "# ---------------------\n",
    "#  Stochastic Variational Inference's Guide\n",
    "# ---------------------\n",
    "def guide(x, y):\n",
    "    priors = {}\n",
    "    for iterator in model.named_parameters():\n",
    "        name, param = iterator\n",
    "        priors[name] = dist.Normal(loc=pyro.param(name + '.mu', torch.randn_like(param)),\n",
    "                                   scale=F.softplus(pyro.param(name + '.sigma', torch.randn_like(param))))\n",
    "\n",
    "    lifted_module = pyro.random_module('module', model, priors)\n",
    "    return lifted_module()\n",
    "\n",
    "\n",
    "opt = optim.Adam({'lr': parameters[\"lr_Classifier\"]})\n",
    "svi = SVI(module, guide, opt, loss=Trace_ELBO())\n",
    "\n",
    "# ---------------------\n",
    "#  Classifier's prediction method\n",
    "# ---------------------\n",
    "def predict(x, y):\n",
    "    sampled_models = [guide(None, None) for _ in range(parameters[\"n_classes\"])]\n",
    "    yhats = [model(x.to(device)).data for model in sampled_models]\n",
    "    mean = torch.mean(torch.stack(yhats), 0)\n",
    "    predsProbs, preds = torch.max(F.softmax(mean).to(device), 1)\n",
    "    return predsProbs, preds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training And Saving Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 2.00 GiB total capacity; 454.56 MiB already allocated; 0 bytes free; 478.00 MiB reserved in total by PyTorch) (malloc at ..\\c10\\cuda\\CUDACachingAllocator.cpp:289)\n(no backtrace available)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-a5a7ebe44630>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ClassifierModel.pt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[0mtrain_classifier_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-5-a5a7ebe44630>\u001b[0m in \u001b[0;36mtrain_classifier_model\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrainloader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m             \u001b[0mloss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0msvi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Epoch: %d Loss: %f\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyro\\infer\\svi.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    126\u001b[0m         \u001b[1;31m# get loss and compute gradients\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mpoutine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam_only\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mparam_capture\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss_and_grads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mguide\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m         params = set(site[\"value\"].unconstrained()\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyro\\infer\\trace_elbo.py\u001b[0m in \u001b[0;36mloss_and_grads\u001b[1;34m(self, model, guide, *args, **kwargs)\u001b[0m\n\u001b[0;32m    135\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtrainable_params\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msurrogate_loss_particle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'requires_grad'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m                 \u001b[0msurrogate_loss_particle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msurrogate_loss_particle\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_particles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 137\u001b[1;33m                 \u001b[0msurrogate_loss_particle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    138\u001b[0m         \u001b[0mwarn_if_nan\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"loss\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m         \"\"\"\n\u001b[1;32m--> 198\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    199\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 100\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 2.00 GiB total capacity; 454.56 MiB already allocated; 0 bytes free; 478.00 MiB reserved in total by PyTorch) (malloc at ..\\c10\\cuda\\CUDACachingAllocator.cpp:289)\n(no backtrace available)"
     ]
    }
   ],
   "source": [
    "# ---------------------\n",
    "#  Trains Classifier Model\n",
    "# ---------------------\n",
    "def train_classifier_model():\n",
    "    pyro.clear_param_store()\n",
    "\n",
    "    total_loss = 0\n",
    "    for epoch in range(parameters[\"n_epochs_Classifier\"]):\n",
    "        loss = 0\n",
    "        for x, y in trainloader:\n",
    "            loss += svi.step(x.flatten(1).to(device), y.to(device))\n",
    "        total_loss = loss / len(trainloader.dataset)\n",
    "        print(\"Epoch: %d Loss: %f\" % (epoch + 1, total_loss))\n",
    "\n",
    "    pyro.get_param_store().save('paramstore.out')\n",
    "    torch.save(model.state_dict(), 'ClassifierModel.pt')\n",
    "\n",
    "\n",
    "# ---------------------\n",
    "#  Trains Generative Adverserial Network model\n",
    "# ---------------------\n",
    "def train_GAN(dataloader):\n",
    "    batches_done = 0\n",
    "    for epoch in range(parameters[\"n_epochs_GAN\"]):\n",
    "        for i, (imgs, labels) in enumerate(dataloader):\n",
    "            hot_labels = create_one_hot_label(labels)\n",
    "\n",
    "            real_images = Variable(imgs.type(FloatTensor))\n",
    "            labels = Variable(labels.type(LongTensor))\n",
    "\n",
    "            discriminator_optimizer.zero_grad()\n",
    "\n",
    "            z = Variable(FloatTensor(np.random.normal(0, 1, (parameters[\"batch_size\"], parameters[\"latent_dim\"]))))\n",
    "\n",
    "            generated_images = generator(z, real_images)\n",
    "\n",
    "            d_loss = calculate_discriminator_loss(real_images, labels, generated_images, hot_labels)\n",
    "\n",
    "            d_loss.backward()\n",
    "            discriminator_optimizer.step()\n",
    "\n",
    "            generator_optimizer.zero_grad()\n",
    "\n",
    "\n",
    "            if i % parameters[\"n_critic\"] == 0:\n",
    "\n",
    "                generated_images = generator(z, real_images)\n",
    "\n",
    "                fake_validity = discriminator(generated_images, hot_labels)\n",
    "                g_loss = -torch.mean(fake_validity)\n",
    "\n",
    "                g_loss.backward()\n",
    "                generator_optimizer.step()\n",
    "\n",
    "                print_progress(epoch, d_loss, g_loss)\n",
    "\n",
    "                batches_done += batches_done + parameters[\"n_critic\"]\n",
    "\n",
    "                if batches_done % 100 == 0:\n",
    "                    save_image(generated_images.data[:25], \"images/wgan/%d.png\" % batches_done, nrow=5, normalize=True)\n",
    "\n",
    "    save_GAN_models()\n",
    "    \n",
    "# ---------------------\n",
    "#  Saves Generator and Discriminator Models\n",
    "# ---------------------\n",
    "def save_GAN_models():\n",
    "    torch.save(generator.state_dict(), 'GeneratorModel.pt')\n",
    "    torch.save(discriminator.state_dict(), 'DiscriminatorModel.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------\n",
    "#  Test for Classifier\n",
    "# ---------------------\n",
    "def test_classifier():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for x, y in testloader:\n",
    "        x, y = arrange_data_tensors(x, y)\n",
    "        predsProbs, preds = predict(x.flatten(1), y)\n",
    "        total += parameters[\"batch_size\"]\n",
    "        correct += (preds == y).sum().item()\n",
    "    print(\"Accuracy: %f\" % (correct / total))\n",
    "\n",
    "test_classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating mask to train model with only critic dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\pyro\\primitives.py:406: FutureWarning: The `random_module` primitive is deprecated, and will be removed in a future release. Use `pyro.nn.Module` to create Bayesian modules from `torch.nn.Module` instances.\n",
      "  \"modules from `torch.nn.Module` instances.\", FutureWarning)\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:66: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "# ---------------------\n",
    "#  Trains or loads classifier model\n",
    "# ---------------------\n",
    "def create_classifier_model():\n",
    "    if (not os.path.exists('ClassifierModel.pt') or not os.path.exists('paramstore.out')):\n",
    "        train_classifier_model()\n",
    "        test_classifier()\n",
    "    else:\n",
    "        pyro.get_param_store().load('paramstore.out')\n",
    "        model.load_state_dict(torch.load('ClassifierModel.pt'))\n",
    "\n",
    "create_classifier_model()\n",
    "mask = []\n",
    "\n",
    "\n",
    "# ---------------------\n",
    "#  Creates Mask To Train Discriminator With P_Critic\n",
    "# ---------------------\n",
    "def create_critic_mask():\n",
    "    print(\"Creating mask to train model with only critic dataset...\")\n",
    "    for i in range(int(len(trainset) / parameters[\"batch_size\"])):\n",
    "        # obtain one batch of training images\n",
    "        dataiter = iter(trainloader)\n",
    "        images, labels = dataiter.next()\n",
    "        images, labels = arrange_data_tensors(images, labels)\n",
    "\n",
    "        predsProbs, preds= predict(images, labels)\n",
    "        # convert output probabilities to predicted class\n",
    "        # predsProbs, preds = torch.max(output, 1)\n",
    "        for j in range(parameters[\"batch_size\"]):\n",
    "            mask.append(1 if predsProbs[j].item() > parameters[\"critic_threshold\"] else 0)\n",
    "\n",
    "\n",
    "create_critic_mask()\n",
    "mask = FloatTensor(mask)\n",
    "\n",
    "\n",
    "# ---------------------\n",
    "#  Sampler Class to use mask\n",
    "# ---------------------\n",
    "class SpecialSampler(torch.utils.data.sampler.Sampler):\n",
    "    def __init__(self, mask, data_source):\n",
    "        self.mask = mask\n",
    "        self.data_source = data_source\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter([i.item() for i in torch.nonzero(mask)])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_source)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------\n",
    "#  Calculates Transformation Loss While Calculating Discriminator Loss\n",
    "# ---------------------\n",
    "def calculate_transformation_loss(img, label):\n",
    "    loss = 0\n",
    "    loss += svi.evaluate_loss(img.flatten(1).to(device), label.to(device))\n",
    "    return loss/(parameters[\"batch_size\"]**2)\n",
    "\n",
    "\n",
    "# ---------------------\n",
    "#  Loads Models Or Trains Classifier, Generator, Discriminator Models\n",
    "# ---------------------\n",
    "def load_or_train_models():\n",
    "    create_classifier_model()\n",
    "\n",
    "    if parameters[\"continue_on_existing_training\"] or (\n",
    "            not (os.path.exists('DiscriminatorModel.pt') or os.path.exists('GeneratorModel.pt'))):\n",
    "        sampler = SpecialSampler(mask, trainset)\n",
    "        special_loader = torch.utils.data.DataLoader(\n",
    "            trainset,\n",
    "            drop_last=True,\n",
    "            batch_size=parameters[\"batch_size\"],\n",
    "            sampler=sampler,\n",
    "            shuffle=False\n",
    "        )\n",
    "        if parameters[\"continue_on_existing_training\"] == True and (\n",
    "                os.path.exists('DiscriminatorModel.pt') and os.path.exists('GeneratorModel.pt')):\n",
    "            load_GAN_models()\n",
    "\n",
    "        train_GAN(special_loader)\n",
    "    else:\n",
    "        load_GAN_models()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------\n",
    "#  Generator Class\n",
    "# ---------------------\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        def block(in_feat, out_feat, normalize=True):\n",
    "            layers = [nn.Linear(in_feat, out_feat)]\n",
    "            if normalize:\n",
    "                layers.append(nn.BatchNorm1d(out_feat, 0.8))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "\n",
    "        self.fc = nn.Linear(parameters[\"latent_dim\"], parameters[\"channels\"] * parameters[\"img_size\"] ** 2)\n",
    "\n",
    "        self.init_size = parameters[\"img_size\"] // 4  # Initial size before upsampling\n",
    "\n",
    "        self.l1 = nn.Sequential(nn.Conv2d(parameters[\"channels\"] * 2, 64, 3, 1, 1), nn.ReLU(inplace=True))\n",
    "\n",
    "        self.conv_blocks_for_image = nn.Sequential(\n",
    "            nn.Conv2d(1, 1, 3, stride=2, padding=1),\n",
    "            nn.Dropout2d(0.1),\n",
    "            nn.BatchNorm2d(1, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "\n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout2d(0.25),\n",
    "            nn.Conv2d(128, 256, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout2d(0.5),\n",
    "            nn.Conv2d(256, 128, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout2d(0.5),\n",
    "            nn.Conv2d(128, 64, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout2d(0.5),\n",
    "            nn.Conv2d(64, 32, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(32, 16, 3, 1, 1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout2d(0.2),\n",
    "            nn.BatchNorm2d(16, 0.8),\n",
    "            nn.Conv2d(16, 1, 3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "        self.l2 = nn.Sequential(\n",
    "            nn.Linear(4 * parameters[\"img_size\"] ** 2, parameters[\"channels\"] * parameters[\"img_size\"] ** 2),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, z, img):\n",
    "        # img = play_with_image(img)\n",
    "        first_z = self.fc(z).view(\n",
    "            [parameters[\"batch_size\"], 1, int(parameters[\"img_size\"]), int(parameters[\"img_size\"])])\n",
    "        gen_input = torch.cat((FloatTensor(img), first_z), 1)\n",
    "        out = self.l1(gen_input)\n",
    "        generated_img = self.conv_blocks(out)\n",
    "        return generated_img\n",
    "\n",
    "\n",
    "# ---------------------\n",
    "#  Discriminator Class\n",
    "# ---------------------\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(int(np.prod(img_shape)) + parameters[\"n_classes\"], 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(256, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, img, labels):\n",
    "        gen_input = torch.cat(\n",
    "            (img.view(parameters[\"img_size\"] * parameters[\"img_size\"], parameters[\"batch_size\"]),\n",
    "             labels.view(parameters[\"n_classes\"], parameters[\"batch_size\"])), 0). \\\n",
    "            view(parameters[\"batch_size\"], parameters[\"img_size\"] * parameters[\"img_size\"] + parameters[\"n_classes\"])\n",
    "        validity = self.model(gen_input)\n",
    "        return validity\n",
    "\n",
    "\n",
    "generator = Generator()\n",
    "discriminator = Discriminator()\n",
    "\n",
    "if cuda:\n",
    "    generator.cuda()\n",
    "    discriminator.cuda()\n",
    "\n",
    "generator_optimizer = torch.optim.Adam(generator.parameters(), lr=parameters[\"lr_GAN\"],\n",
    "                                       betas=(parameters[\"b1\"], parameters[\"b2\"]))\n",
    "discriminator_optimizer = torch.optim.Adam(discriminator.parameters(), lr=parameters[\"lr_GAN\"],\n",
    "                                           betas=(parameters[\"b1\"], parameters[\"b2\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------\n",
    "#  Computes Gradient Penalty\n",
    "# ---------------------\n",
    "def compute_gradient_penalty(discriminator, real_samples, fake_samples, hot_labels):\n",
    "    alpha = FloatTensor(np.random.random((real_samples.size(0), 1, 1, 1)))\n",
    "    interpolates = (alpha * real_samples + ((1 - alpha) * fake_samples)).requires_grad_(True)\n",
    "    gradients = autograd.grad(\n",
    "        discriminator(interpolates, hot_labels),\n",
    "        interpolates,\n",
    "        Variable(FloatTensor(parameters[\"batch_size\"], 1).fill_(1.0), requires_grad=False),\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "        only_inputs=True,\n",
    "    )\n",
    "    gradients = gradients[0].view(gradients[0].size(0), -1)\n",
    "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "    return gradient_penalty\n",
    "\n",
    "# ---------------------\n",
    "#  Creates One Hot Label Representation\n",
    "# ---------------------\n",
    "def create_one_hot_label(labels):\n",
    "    hot_labels = []\n",
    "    hot_label = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "    for i in range(len(labels)):\n",
    "        hot_label[labels[i]] = 1\n",
    "        hot_labels.append(hot_label)\n",
    "\n",
    "    return FloatTensor(hot_labels)\n",
    "\n",
    "\n",
    "# ---------------------\n",
    "#  Calculates Discriminator Loss\n",
    "# ---------------------\n",
    "def calculate_discriminator_loss(real_images, real_labels, fake_images, hot_labels):\n",
    "    real_validity = discriminator(real_images, hot_labels)\n",
    "    fake_validity = discriminator(fake_images, hot_labels)\n",
    "\n",
    "    # Calculating gradient penalty\n",
    "    gradient_penalty = compute_gradient_penalty(discriminator, real_images.data, fake_images.data, hot_labels)\n",
    "    # Calculating transformation penalty\n",
    "    transformation_loss = calculate_transformation_loss(fake_images, real_labels)\n",
    "\n",
    "    # Calculating total penalty\n",
    "    total_loss = -torch.mean(real_validity) + \\\n",
    "                 torch.mean(fake_validity) + \\\n",
    "                 parameters[\"lambda_gp\"] * gradient_penalty + \\\n",
    "                 parameters[\"lambda_loss\"] * transformation_loss\n",
    "\n",
    "    return total_loss\n",
    "\n",
    "\n",
    "# ---------------------\n",
    "#  Prints GAN progress\n",
    "# ---------------------\n",
    "def print_progress(epoch, d_loss, g_loss):\n",
    "    print(\"[Epoch %d/%d] [D loss: %f] [G loss: %f]\"\n",
    "          % (epoch, parameters[\"n_epochs_GAN\"], d_loss.item(), g_loss.item())\n",
    "          )\n",
    "\n",
    "    \n",
    "# ---------------------\n",
    "#  Converts Images and Labels\n",
    "# ---------------------\n",
    "def arrange_data_tensors(images, labels):\n",
    "    images = Variable(images.type(FloatTensor))\n",
    "    labels = Variable(labels.type(LongTensor))\n",
    "\n",
    "    return images, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ---------------------\n",
    "#  Loads Generator and Discriminator Models\n",
    "# ---------------------\n",
    "def load_GAN_models():\n",
    "    generator.load_state_dict(torch.load('GeneratorModel.pt'))\n",
    "    discriminator.load_state_dict(torch.load('DiscriminatorModel.pt'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------\n",
    "#  Generates image from generator and saves if save parameter is true\n",
    "# ---------------------\n",
    "def sample_single_image(images, index, image_path, save):\n",
    "    z = Variable(FloatTensor(np.random.normal(0, 1, (parameters[\"batch_size\"], parameters[\"latent_dim\"]))))\n",
    "    generated_images = generator(z, images)\n",
    "    if (save == True):\n",
    "        save_image(generated_images[index].data, image_path, nrow=1, normalize=True)\n",
    "\n",
    "    return generated_images[index].data\n",
    "\n",
    "\n",
    "# ---------------------\n",
    "#  Shows the given image\n",
    "# ---------------------\n",
    "def show_image(img):\n",
    "    img = img.view(parameters[\"img_size\"], parameters[\"img_size\"])\n",
    "    img = img.type(torch.FloatTensor).detach().numpy()\n",
    "    plt.imshow(img, cmap='gray')\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Models And Printing Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test_models produces quantitive and qualitive results.\n",
    "\n",
    "1)Quantitive result is generating a handwritten number which has a certainty that is under threshold.(Figure 2 from Paper)\n",
    "\n",
    "2)Qualitive result is printing three rows of the \"Table 1\" from the paper.\n",
    "It produces results for [0.7, 0.8, 0.9] thresholds (Three rows of the Table 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating images and printing results...\n",
      "-------------------Results for Threshold: 0.700000 -------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:66: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Qualitative Results for Threshold----------\n",
      "Original Image : 0.640706 (Prediction Acc) || Old Label:  2\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Image: 1.000000 (Prediction Acc) || New Label:  8\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAOe0lEQVR4nO3db6gd9Z3H8c/HpFGTRkj8t8Ema1PzQFlcswRZtCxdyhbjk1ik0jxYsq7u7YMKLeyDFfdBhKUgy7ZLQSncrNJkrZaCFmMpNCGW6j6wGCVqbLomKzFNE5KNgkn9Q/7c7z44k+U23jNzMnPmzCTf9wsu5575nTPzvXPv587M+c3MzxEhABe/S7ouAMBkEHYgCcIOJEHYgSQIO5DE/EkuzDYf/QMtiwjPNb1R2G3fIen7kuZJ+o+IeKTJ/PrMnnP9SZKadl+Wzbttfe56rVovfa69TXX/FmvvxtueJ+kxSWsl3SRpve2b6s4PQLuaHLPfKmlfRLwTEScl/VjSuvGUBWDcmoT9Okm/m/X8YDHtj9iesr3T9s4GywLQUJNj9rkOHD51wBAR05KmJT6gA7rUZMt+UNLyWc8/J+lQs3IAtKVJ2F+RtMr2520vkPR1SVvHUxaAcau9Gx8Rp20/IOkXGnS9PRERb42tsp5ps5snaxdSFdbL3OquF09yhXLMDrRv2Ek1nC4LJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EASE72VtNTsTqpc8ji3Nu9O2+Y6b/uuuhfq30vVeilrn5mZGdrGlh1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkph4P/uFqsvzA5r0uzbV5UiqfR7dtmltZfNva52yZQeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJHrVz97n64/73J9cVlvVvFesWFHa/thjj5W2P/roo6XtL7zwwtC2M2fOlL63aV932bXdZW2jaLsfvsmyh2kUdtv7JZ2QdEbS6YhY02R+ANozji37X0fEsTHMB0CLOGYHkmga9pC0zfartqfmeoHtKds7be9suCwADTTdjb89Ig7ZvkbSdtu/jYgXZ78gIqYlTUuS7f5+Agdc5Bpt2SPiUPF4VNJPJd06jqIAjF/tsNteZHvx2e8lfUXS7nEVBmC8XLfPzvZKDbbm0uBw4KmI+E7Fe6Kt68K7vPa57evNq/qEL7lk+P/s06dPN1r2xWrNmvJe4tdff720veocgSbnADQVEXMuvPYxe0S8I+nPa1cEYKLoegOSIOxAEoQdSIKwA0kQdiCJ2l1vtRbW4663JpcslnV9jWPZ8+bNK20/efJko+Xj/C1YsKC0varLs83cDet6Y8sOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0n06lbSfdbmJbS33XZbaftLL73U2rLbdu+99w5tW7lyZel7t2zZUtr+wQcflLYfPXq0tL2J559/vrR97dq1rS27LrbsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5BEmuvZm/6c8+cPPyWh6W2Fq97fpssvv7y0/ZNPPmlt2W3/zrocArzqHgRd3EqaLTuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJMH17CMquzd8l/3kTTXtR29yz/wu+8Hb1sefrfI3ZfsJ20dt7541bant7bb3Fo9L2i0TQFOj/Fv+oaQ7zpn2oKQdEbFK0o7iOYAeqwx7RLwo6f1zJq+TtLn4frOku8ZcF4Axq3vMfm1EHJakiDhs+5phL7Q9JWmq5nIAjEnrH9BFxLSkaWlwIUzbywMwt7ofpR6xvUySisf2buMJYCzqhn2rpA3F9xskPTeecgC0pXI33vbTkr4k6SrbByVtlPSIpJ/Yvk/SAUlfa7PIPuhjv+kobrzxxlbn3+Z12VW2b9/e2bLffvvtzpZdV2XYI2L9kKYvj7kWAC3idFkgCcIOJEHYgSQIO5AEYQeS4BLXEZVdxlp1S+Sq9htuuKG0fd++faXtZVavXl3afujQodL248eP1152larLY/t86fDNN99c2t72bbLrYMsOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kwZPOIyuZftewFCxaUtp8+fbq0vWy4aEm6++67h7Y9+eSTpe9tatWqVaXtBw4cGNpWdRvrJn8rTV122WWl7VW/syptnkPAkM1AcoQdSIKwA0kQdiAJwg4kQdiBJAg7kESv+tkv1Ns1V/UHt33d9ooVK4a2vfvuu43mfSEru09A1Xqp+p3Mmzev0fvb/Funnx1IjrADSRB2IAnCDiRB2IEkCDuQBGEHkrho7hvf9NrnC7WPX5LuueeerkuoZeHChaXtp06dajT/sr7uqnMfqv6euuxHr6tyy277CdtHbe+eNe1h27+3vav4urPdMgE0Ncpu/A8l3THH9H+PiFuKr5+PtywA41YZ9oh4UdL7E6gFQIuafED3gO03it38JcNeZHvK9k7bOxssC0BDdcP+A0lfkHSLpMOSvjvshRExHRFrImJNzWUBGINaYY+IIxFxJiJmJG2SdOt4ywIwbrXCbnvZrKdflbR72GsB9EPl9ey2n5b0JUlXSToiaWPx/BZJIWm/pG9ExOHKhdmtdT5W9ZtWmZmZKW0v63e94oorSt+7e3f5/8Lp6enS9m3btpW2v/zyy6XtfdX03IgmfeUXcj961T0hhl3PXnlSTUSsn2Py46OXBqAPOF0WSIKwA0kQdiAJwg4kQdiBJC6aW0k37cZZunRpafuxY8cazb+Jqm6gsiGdN23aVPre+++/v1ZN41D1O6tqrxrKumy9Vf2t9fES1bPqdr2xZQeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJNL0s1966aWl7R9//HHtZbet6TkEZbrsT256O+cqTX62ES79bm3ZVehnB1CKsANJEHYgCcIOJEHYgSQIO5AEYQeSmPiQzW31P1bNt+pW0X1W1ae7ZMnQ0bf03nvvjbucsbmYh9nush9+GLbsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5DExPvZu3Lq1KmuS6jtQj5HYPHixUPbml4z3kQf+8HbVrllt73c9i9t77H9lu1vFdOX2t5ue2/xOPzMDgCdG2U3/rSkf4yIGyX9paRv2r5J0oOSdkTEKkk7iucAeqoy7BFxOCJeK74/IWmPpOskrZO0uXjZZkl3tVUkgObO65jd9vWSVkv6taRrI+KwNPiHYPuaIe+ZkjTVrEwATY0cdtuflfSMpG9HxPFRPzyJiGlJ08U8Lr5PPYALxEhdb7Y/o0HQfxQRzxaTj9heVrQvk3S0nRIBjEPllt2DTfjjkvZExPdmNW2VtEHSI8Xjc6MssK1bSVepmveiRYtK2z/88MPay964cWNp+1NPPVXavnfv3trLbtvVV19d2v7RRx8Nbau6lXSfu7/a7LprMpR12XJH2Y2/XdLfSnrT9q5i2kMahPwntu+TdEDS10aYF4COVIY9Iv5L0rB/JV8ebzkA2sLpskAShB1IgrADSRB2IAnCDiSRZsjmKl1ebjl/fnmnyMKFC0vbr7zyyqFtVbeSPnHiRGl71eW1bfUJj9LeRNN+coZsBtBbhB1IgrADSRB2IAnCDiRB2IEkCDuQBP3sY5h/232uXfY3N1VWe9Ofq83aq2prei1+WXuTcxdmZmboZweyI+xAEoQdSIKwA0kQdiAJwg4kQdiBJHo1ZHOT/uo+32O8y9ravs6/7fc3mXeX/fBtqrtstuxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kETl9ey2l0vaIulPJM1Imo6I79t+WNI/SPrf4qUPRcTPK+bV385w4CIx7Hr2UcK+TNKyiHjN9mJJr0q6S9I9kv4QEf82ahGEHWjfsLCPMj77YUmHi+9P2N4j6brxlgegbed1zG77ekmrJf26mPSA7TdsP2F7yZD3TNneaXtno0oBNDLyPehsf1bSryR9JyKetX2tpGOSQtK/aLCr//cV82A3HmhZ7WN2SbL9GUk/k/SLiPjeHO3XS/pZRPxZxXwIO9Cy2jec9ODSoccl7Zkd9OKDu7O+Kml30yIBtGeUT+O/KOklSW9q0PUmSQ9JWi/pFg124/dL+kbxYV7ZvFq7lTSAgUa78eNC2IH2cd94IDnCDiRB2IEkCDuQBGEHkiDsQBITv5U0XW9AM3UzxJYdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5KYdD/7sZmZmXdnPb9Kg1tb9VFfa+trXRK11XVetVWcj/Knwxomej37pxZu74yINZ0VUKKvtfW1Lona6ppUbezGA0kQdiCJrsM+3fHyy/S1tr7WJVFbXROprdNjdgCT0/WWHcCEEHYgiU7CbvsO2/9te5/tB7uoYRjb+22/aXtX1+PTFWPoHbW9e9a0pba3295bPM45xl5HtT1s+/fFuttl+86Oaltu+5e299h+y/a3iumdrruSuiay3iZ+zG57nqS3Jf2NpIOSXpG0PiJ+M9FChrC9X9KaiOj8BAzbfyXpD5K2nB1ay/a/Sno/Ih4p/lEuiYh/6kltD+s8h/FuqbZhw4z/nTpcd+Mc/ryOLrbst0raFxHvRMRJST+WtK6DOnovIl6U9P45k9dJ2lx8v1mDP5aJG1JbL0TE4Yh4rfj+hKSzw4x3uu5K6pqILsJ+naTfzXp+UP0a7z0kbbP9qu2prouZw7Vnh9kqHq/puJ5zVQ7jPUnnDDPem3VXZ/jzproI+1w30OpT/9/tEfEXktZK+maxu4rR/EDSFzQYA/CwpO92WUwxzPgzkr4dEce7rGW2OeqayHrrIuwHJS2f9fxzkg51UMecIuJQ8XhU0k81OOzokyNnR9AtHo92XM//i4gjEXEmImYkbVKH664YZvwZST+KiGeLyZ2vu7nqmtR66yLsr0haZfvzthdI+rqkrR3U8Sm2FxUfnMj2IklfUf+Got4qaUPx/QZJz3VYyx/pyzDew4YZV8frrvPhzyNi4l+S7tTgE/n/kfTPXdQwpK6Vkl4vvt7qujZJT2uwW3dKgz2i+yRdKWmHpL3F49Ie1fafGgzt/YYGwVrWUW1f1ODQ8A1Ju4qvO7tedyV1TWS9cboskARn0AFJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEv8H4UBkRr1QPEgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Qualitative Results for Threshold----------\n",
      "\n",
      "----------Quantitative Results for Threshold----------\n",
      "Reject            : 0.100000\n",
      "BNN Accuracy      : 47.000000\n",
      "BNN+GWIN Accuracy : 50.000000\n",
      "Rejected Accuracy :  3\n",
      "Overall Accuracy  : 0.003000\n",
      "Error             : 3.000000\n",
      "----------Quantitative Results for Threshold----------\n",
      "-------------------Results for Threshold: 0.700000 -------------------\n",
      "\n",
      "\n",
      "-------------------Results for Threshold: 0.800000 -------------------\n",
      "----------Qualitative Results for Threshold----------\n",
      "Original Image : 0.585931 (Prediction Acc) || Old Label:  9\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Image: 1.000000 (Prediction Acc) || New Label:  4\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Qualitative Results for Threshold----------\n",
      "\n",
      "----------Quantitative Results for Threshold----------\n",
      "Reject            : 0.166000\n",
      "BNN Accuracy      : 37.951807\n",
      "BNN+GWIN Accuracy : 46.987952\n",
      "Rejected Accuracy :  9\n",
      "Overall Accuracy  : 0.015000\n",
      "Error             : 5.443461\n",
      "----------Quantitative Results for Threshold----------\n",
      "-------------------Results for Threshold: 0.800000 -------------------\n",
      "\n",
      "\n",
      "-------------------Results for Threshold: 0.900000 -------------------\n",
      "----------Qualitative Results for Threshold----------\n",
      "Original Image : 0.593226 (Prediction Acc) || Old Label:  3\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Image: 1.000000 (Prediction Acc) || New Label:  2\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Qualitative Results for Threshold----------\n",
      "\n",
      "----------Quantitative Results for Threshold----------\n",
      "Reject            : 0.247000\n",
      "BNN Accuracy      : 43.724696\n",
      "BNN+GWIN Accuracy : 50.607287\n",
      "Rejected Accuracy :  6\n",
      "Overall Accuracy  : 0.017000\n",
      "Error             : 2.786474\n",
      "----------Quantitative Results for Threshold----------\n",
      "-------------------Results for Threshold: 0.900000 -------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ---------------------\n",
    "#  Test\n",
    "# ---------------------\n",
    "def test_models():\n",
    "    load_or_train_models()\n",
    "    print(\"Generating images and printing results...\")\n",
    "    for threshold_index in range(len(parameters[\"threshold\"])):\n",
    "        print('-------------------Results for Threshold: %2f -------------------' % parameters[\"threshold\"][threshold_index])\n",
    "        counter = 0\n",
    "        probUnderThreshold = 0\n",
    "        probUnderThresholdButCorrectClassified = 0\n",
    "        probUnderThresholdThenClassifiedCorrectClassified = 0\n",
    "        probAboveThresholdAndCorrectClassified = 0\n",
    "        probAboveThresholdAlsoClassifiedCorrectClassified = 0\n",
    "\n",
    "        for i in range(10):\n",
    "            for j in range(int(len(testset) / parameters[\"batch_size\"])):\n",
    "                # obtain one batch of training images\n",
    "                dataiter = iter(testloader)\n",
    "                images, labels = dataiter.next()\n",
    "\n",
    "                images, labels = arrange_data_tensors(images, labels)\n",
    "\n",
    "                # get sample outputs\n",
    "                output = model(images)\n",
    "\n",
    "                predsProbs, preds = predict(images, labels)\n",
    "\n",
    "                # convert output probabilities to predicted class\n",
    "                # predsProbs, preds = torch.max(output, 1)\n",
    "\n",
    "                for k in range(parameters[\"batch_size\"]):\n",
    "\n",
    "                    idx = (i * int(len(testset))) + (j * parameters[\"batch_size\"]) + k\n",
    "\n",
    "                    trueLabel = labels[k].item()\n",
    "                    modelsPrediction = preds[k].item()\n",
    "                    modelsPredictionProb = predsProbs[k].item()\n",
    "\n",
    "                    if (modelsPredictionProb < parameters[\"threshold\"][threshold_index]):\n",
    "                        probUnderThreshold = probUnderThreshold + 1\n",
    "                        if (trueLabel == modelsPrediction):\n",
    "                            probUnderThresholdButCorrectClassified = probUnderThresholdButCorrectClassified + 1\n",
    "                    else:\n",
    "                        if (trueLabel == modelsPrediction):\n",
    "                            probAboveThresholdAndCorrectClassified = probAboveThresholdAndCorrectClassified + 1\n",
    "\n",
    "                    if (predsProbs[k] < parameters[\"threshold\"][threshold_index]):\n",
    "                        save_image(images[k], \"images/result_images/%d\" % idx + \"_original_image.png\", nrow=1,\n",
    "                                   normalize=True)\n",
    "\n",
    "                        generatedImage = sample_single_image(images, k,\n",
    "                                                             \"images/result_images/%d\" % idx + \"_generated_image_.png\",\n",
    "                                                             True)\n",
    "                        lastPredsProbs, lastPreds = predict(generatedImage.view(1, 1, 28, 28), labels)\n",
    "\n",
    "                        if not modelsPrediction == trueLabel and lastPreds.item() == trueLabel and counter < 1:\n",
    "                            print('----------Qualitative Results for Threshold----------')\n",
    "                            print('Original Image : %2f (Prediction Acc) || Old Label: %2d' % (predsProbs[k], preds[k]))\n",
    "                            show_image(images[k])\n",
    "                            print(\"Generated Image: %2f (Prediction Acc) || New Label: %2d\" % (lastPredsProbs.item(), lastPreds.item()))\n",
    "                            show_image(generatedImage)\n",
    "                            counter = counter + 1\n",
    "                            print('----------Qualitative Results for Threshold----------\\n')\n",
    "\n",
    "                        if (lastPreds.item() == trueLabel):\n",
    "                            probUnderThresholdThenClassifiedCorrectClassified = probUnderThresholdThenClassifiedCorrectClassified + 1\n",
    "\n",
    "        total_images = 100000.0\n",
    "\n",
    "        reject = (float(100 * probUnderThreshold) / total_images)\n",
    "        bnnAcc = (100 * probUnderThresholdButCorrectClassified / probUnderThreshold)\n",
    "        bnn_and_gwinAcc = (100 * probUnderThresholdThenClassifiedCorrectClassified / probUnderThreshold)\n",
    "        overallAcc = (float(100 * (\n",
    "                probUnderThresholdThenClassifiedCorrectClassified - probUnderThresholdButCorrectClassified)) / total_images)\n",
    "        error = (100 * (bnn_and_gwinAcc - bnnAcc)) / probUnderThreshold\n",
    "\n",
    "        print('----------Quantitative Results for Threshold----------')\n",
    "        print('Reject            : %2f' % reject)\n",
    "        print('BNN Accuracy      : %2f' % bnnAcc)\n",
    "        print('BNN+GWIN Accuracy : %2f' % bnn_and_gwinAcc)\n",
    "        print('Rejected Accuracy : %2d' % (bnn_and_gwinAcc - bnnAcc))\n",
    "        print('Overall Accuracy  : %2f' % overallAcc)\n",
    "        print('Error             : %2f' % error)\n",
    "        print('----------Quantitative Results for Threshold----------')\n",
    "\n",
    "        print('-------------------Results for Threshold: %2f -------------------\\n\\n' % parameters[\"threshold\"][threshold_index])\n",
    "\n",
    "        \n",
    "test_models()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
