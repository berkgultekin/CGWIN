{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paper Information :\n",
    "    Generative Well-intentioned Networks \n",
    "    https://papers.nips.cc/paper/9467-generative-well-intentioned-networks.pdf\n",
    "\n",
    "Authors of code:\n",
    "    Yasin Berk GÃ¼ltekin - 1942119\n",
    "    Hasan Ali Duran - 1942119\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenges Encountered When Implementing Paper:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* There was not enough detail about how BNN was implemented. The number of layers was not specified. \n",
    "We could not fully obtain the BNN results mentioned in Paper. The BNN we have implemented makes predictions with higher scores. \n",
    "This situation caused difficulties in the exact occurrence of qualitative results.\n",
    "\n",
    "* The biggest problem we encountered during GAN implementation was that it was not clear enough how the\n",
    "Generator and Discriminator inputs should be processed in the model. It was not specified how many layers \n",
    "or what types of layers were used. In addition, the pictures produced by the generator appeared similar to those given\n",
    "as input to the Generator. There was no explanation for how this problem was solved in paper implementation. The new method which is transformation loss used during the Discriminator's loss calculation was not sufficiently explained.(You can find our assumptions for the models(like number of layers and type of layers) in the implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-user install because site-packages writeable\n",
      "Created temporary directory: C:\\Users\\Lenovo\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-x0ausrdi\n",
      "Created temporary directory: C:\\Users\\Lenovo\\AppData\\Local\\Temp\\pip-req-tracker-ntczw9ee\n",
      "Initialized build tracking at C:\\Users\\Lenovo\\AppData\\Local\\Temp\\pip-req-tracker-ntczw9ee\n",
      "Created build tracker: C:\\Users\\Lenovo\\AppData\\Local\\Temp\\pip-req-tracker-ntczw9ee\n",
      "Entered build tracker: C:\\Users\\Lenovo\\AppData\\Local\\Temp\\pip-req-tracker-ntczw9ee\n",
      "Created temporary directory: C:\\Users\\Lenovo\\AppData\\Local\\Temp\\pip-install-vmsntved\n",
      "Requirement already satisfied: pyro-ppl in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (1.3.1)\n",
      "Requirement already satisfied: pyro-api>=0.1.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from pyro-ppl) (0.1.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from pyro-ppl) (3.2.1)\n",
      "Requirement already satisfied: tqdm>=4.36 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from pyro-ppl) (4.42.1)\n",
      "Requirement already satisfied: torch>=1.4.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from pyro-ppl) (1.5.0)\n",
      "Requirement already satisfied: numpy>=1.7 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from pyro-ppl) (1.18.1)\n",
      "Requirement already satisfied: future in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from torch>=1.4.0->pyro-ppl) (0.18.2)\n",
      "Cleaning up...\n",
      "Removed build tracker: 'C:\\\\Users\\\\Lenovo\\\\AppData\\\\Local\\\\Temp\\\\pip-req-tracker-ntczw9ee'\n"
     ]
    }
   ],
   "source": [
    "!pip install -v pyro-ppl\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets\n",
    "import os.path\n",
    "import os\n",
    "import numpy as np\n",
    "from torchvision.utils import save_image\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.autograd as autograd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import subprocess\n",
    "import pyro.distributions as dist\n",
    "import pyro\n",
    "from matplotlib import colors\n",
    "from pyro import optim\n",
    "from pyro.infer import SVI, Trace_ELBO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HyperParameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    \"threshold\": [0.7, 0.8, 0.9],\n",
    "    \"critic_threshold\": 0.95,\n",
    "    \"n_critic\": 5,\n",
    "    \"n_epochs_Classifier\": 30,\n",
    "    \"n_epochs_GAN\": 200000,\n",
    "    \"batch_size\": 128,\n",
    "    \"lr_Classifier\": 0.001, # learning rate for Classifier model\n",
    "    \"lr_GAN\": 0.0001,  # learning rate for GAN models\n",
    "    \"b1\": 0.5,\n",
    "    \"b2\": 0.9,\n",
    "    \"latent_dim\": 100,\n",
    "    \"n_classes\": 10,\n",
    "    \"img_size\": 28,\n",
    "    \"channels\": 1,\n",
    "    \"lambda_gp\": 10, # lambda for gradient penalty\n",
    "    \"lambda_loss\": 10, # lambda for transformation penalty\n",
    "    \"continue_on_existing_training\": 0,\n",
    "    \"cuda\": 1,\n",
    "    \"run_download_sh\": 0\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_shape = (parameters[\"channels\"], parameters[\"img_size\"], parameters[\"img_size\"])\n",
    "\n",
    "cuda = True if parameters[\"cuda\"] else False\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "FloatTensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if cuda else torch.LongTensor\n",
    "\n",
    "# ---------------------\n",
    "#  Setting Datasets\n",
    "# ---------------------\n",
    "if parameters[\"run_download_sh\"]:\n",
    "    subprocess.call(\"download.sh\", shell=True)\n",
    "\n",
    "trainset = datasets.MNIST(\n",
    "    root=\"\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.Compose(\n",
    "        [transforms.Resize(parameters[\"img_size\"]), transforms.ToTensor(), transforms.Normalize([0.5], [0.5])]))\n",
    "\n",
    "testset = datasets.MNIST(\n",
    "    root=\"\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transforms.Compose(\n",
    "        [transforms.Resize(parameters[\"img_size\"]), transforms.ToTensor(), transforms.Normalize([0.5], [0.5])]))\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset,\n",
    "    drop_last=True,\n",
    "    batch_size=parameters[\"batch_size\"],\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset,\n",
    "    drop_last=True,\n",
    "    batch_size=parameters[\"batch_size\"],\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classifier Model (Bayesian Neural Network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------\n",
    "#  BNN - Classifier Model\n",
    "# ---------------------\n",
    "class BNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BNN, self).__init__()\n",
    "\n",
    "        self.out = nn.Sequential(\n",
    "            nn.Linear(parameters[\"img_size\"] ** 2, 1024),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(1024, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.flatten(1)\n",
    "        return self.out(x)\n",
    "\n",
    "\n",
    "model = BNN()\n",
    "if cuda:\n",
    "    model.cuda()\n",
    "\n",
    "\n",
    "# ---------------------\n",
    "#  Stochastic Variational Inference's Module\n",
    "# ---------------------\n",
    "def module(x, y):\n",
    "    priors = {}\n",
    "    for iterator in model.named_parameters():\n",
    "        name, param = iterator\n",
    "        zeros = torch.zeros_like(param.data)\n",
    "        ones = torch.ones_like(param.data)\n",
    "        priors[name] = dist.Normal(loc=zeros,\n",
    "                                   scale=ones)\n",
    "\n",
    "    lifted_module = pyro.random_module(\"module\", model, priors)\n",
    "    lifted_module_method = lifted_module()\n",
    "    lhat = F.log_softmax(lifted_module_method(x), 1)\n",
    "    pyro.sample(\"obs\", dist.Categorical(logits=lhat), obs=y)\n",
    "\n",
    "\n",
    "# ---------------------\n",
    "#  Stochastic Variational Inference's Guide\n",
    "# ---------------------\n",
    "def guide(x, y):\n",
    "    priors = {}\n",
    "    for iterator in model.named_parameters():\n",
    "        name, param = iterator\n",
    "        priors[name] = dist.Normal(loc=pyro.param(name + '.mu', torch.randn_like(param)),\n",
    "                                   scale=F.softplus(pyro.param(name + '.sigma', torch.randn_like(param))))\n",
    "\n",
    "    lifted_module = pyro.random_module('module', model, priors)\n",
    "    return lifted_module()\n",
    "\n",
    "\n",
    "opt = optim.Adam({'lr': parameters[\"lr_Classifier\"]})\n",
    "svi = SVI(module, guide, opt, loss=Trace_ELBO())\n",
    "\n",
    "# ---------------------\n",
    "#  Classifier's prediction method\n",
    "# ---------------------\n",
    "def predict(x, y):\n",
    "    sampled_models = [guide(None, None) for _ in range(parameters[\"n_classes\"])]\n",
    "    yhats = [model(x.to(device)).data for model in sampled_models]\n",
    "    mean = torch.mean(torch.stack(yhats), 0)\n",
    "    predsProbs, preds = torch.max(F.softmax(mean).to(device), 1)\n",
    "    return predsProbs, preds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training And Saving Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Loss: 6414.542196\n",
      "Epoch: 2 Loss: 5750.218853\n",
      "Epoch: 3 Loss: 5376.938865\n"
     ]
    }
   ],
   "source": [
    "# ---------------------\n",
    "#  Trains Classifier Model\n",
    "# ---------------------\n",
    "def train_classifier_model():\n",
    "    pyro.clear_param_store()\n",
    "\n",
    "    total_loss = 0\n",
    "    for epoch in range(parameters[\"n_epochs_Classifier\"]):\n",
    "        loss = 0\n",
    "        for x, y in trainloader:\n",
    "            loss += svi.step(x.flatten(1).to(device), y.to(device))\n",
    "        total_loss = loss / len(trainloader.dataset)\n",
    "        print(\"Epoch: %d Loss: %f\" % (epoch + 1, total_loss))\n",
    "\n",
    "    pyro.get_param_store().save('paramstore.out')\n",
    "    torch.save(model.state_dict(), 'ClassifierModel.pt')\n",
    "\n",
    "train_classifier_model()\n",
    "\n",
    "# ---------------------\n",
    "#  Trains Generative Adverserial Network model\n",
    "# ---------------------\n",
    "def train_GAN(dataloader):\n",
    "    batches_done = 0\n",
    "    for epoch in range(parameters[\"n_epochs_GAN\"]):\n",
    "        for i, (imgs, labels) in enumerate(dataloader):\n",
    "            hot_labels = create_one_hot_label(labels)\n",
    "\n",
    "            real_images = Variable(imgs.type(FloatTensor))\n",
    "            labels = Variable(labels.type(LongTensor))\n",
    "\n",
    "            discriminator_optimizer.zero_grad()\n",
    "\n",
    "            z = Variable(FloatTensor(np.random.normal(0, 1, (parameters[\"batch_size\"], parameters[\"latent_dim\"]))))\n",
    "\n",
    "            generated_images = generator(z, real_images)\n",
    "\n",
    "            d_loss = calculate_discriminator_loss(real_images, labels, generated_images, hot_labels)\n",
    "\n",
    "            d_loss.backward()\n",
    "            discriminator_optimizer.step()\n",
    "\n",
    "            generator_optimizer.zero_grad()\n",
    "\n",
    "\n",
    "            if i % parameters[\"n_critic\"] == 0:\n",
    "\n",
    "                generated_images = generator(z, real_images)\n",
    "\n",
    "                fake_validity = discriminator(generated_images, hot_labels)\n",
    "                g_loss = -torch.mean(fake_validity)\n",
    "\n",
    "                g_loss.backward()\n",
    "                generator_optimizer.step()\n",
    "\n",
    "                print_progress(epoch, d_loss, g_loss)\n",
    "\n",
    "                batches_done += batches_done + parameters[\"n_critic\"]\n",
    "\n",
    "                if batches_done % 100 == 0:\n",
    "                    save_image(generated_images.data[:25], \"images/wgan/%d.png\" % batches_done, nrow=5, normalize=True)\n",
    "\n",
    "    save_GAN_models()\n",
    "    \n",
    "# ---------------------\n",
    "#  Saves Generator and Discriminator Models\n",
    "# ---------------------\n",
    "def save_GAN_models():\n",
    "    torch.save(generator.state_dict(), 'GeneratorModel.pt')\n",
    "    torch.save(discriminator.state_dict(), 'DiscriminatorModel.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:66: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.857973\n"
     ]
    }
   ],
   "source": [
    "# ---------------------\n",
    "#  Test for Classifier\n",
    "# ---------------------\n",
    "def test_classifier():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for x, y in testloader:\n",
    "        x, y = arrange_data_tensors(x, y)\n",
    "        predsProbs, preds = predict(x.flatten(1), y)\n",
    "        total += parameters[\"batch_size\"]\n",
    "        correct += (preds == y).sum().item()\n",
    "    print(\"Accuracy: %f\" % (correct / total))\n",
    "\n",
    "test_classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating mask to train model with only critic dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\pyro\\primitives.py:406: FutureWarning: The `random_module` primitive is deprecated, and will be removed in a future release. Use `pyro.nn.Module` to create Bayesian modules from `torch.nn.Module` instances.\n",
      "  \"modules from `torch.nn.Module` instances.\", FutureWarning)\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:66: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "# ---------------------\n",
    "#  Trains or loads classifier model\n",
    "# ---------------------\n",
    "def create_classifier_model():\n",
    "    if (not os.path.exists('ClassifierModel.pt') or not os.path.exists('paramstore.out')):\n",
    "        train_classifier_model()\n",
    "        test_classifier()\n",
    "    else:\n",
    "        pyro.get_param_store().load('paramstore.out')\n",
    "        model.load_state_dict(torch.load('ClassifierModel.pt'))\n",
    "\n",
    "create_classifier_model()\n",
    "mask = []\n",
    "\n",
    "\n",
    "# ---------------------\n",
    "#  Creates Mask To Train Discriminator With P_Critic\n",
    "# ---------------------\n",
    "def create_critic_mask():\n",
    "    print(\"Creating mask to train model with only critic dataset...\")\n",
    "    for i in range(int(len(trainset) / parameters[\"batch_size\"])):\n",
    "        # obtain one batch of training images\n",
    "        dataiter = iter(trainloader)\n",
    "        images, labels = dataiter.next()\n",
    "        images, labels = arrange_data_tensors(images, labels)\n",
    "\n",
    "        predsProbs, preds= predict(images, labels)\n",
    "        # convert output probabilities to predicted class\n",
    "        # predsProbs, preds = torch.max(output, 1)\n",
    "        for j in range(parameters[\"batch_size\"]):\n",
    "            mask.append(1 if predsProbs[j].item() > parameters[\"critic_threshold\"] else 0)\n",
    "\n",
    "\n",
    "create_critic_mask()\n",
    "mask = FloatTensor(mask)\n",
    "\n",
    "\n",
    "# ---------------------\n",
    "#  Sampler Class to use mask\n",
    "# ---------------------\n",
    "class SpecialSampler(torch.utils.data.sampler.Sampler):\n",
    "    def __init__(self, mask, data_source):\n",
    "        self.mask = mask\n",
    "        self.data_source = data_source\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter([i.item() for i in torch.nonzero(mask)])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_source)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------\n",
    "#  Calculates Transformation Loss While Calculating Discriminator Loss\n",
    "# ---------------------\n",
    "def calculate_transformation_loss(img, label):\n",
    "    loss = 0\n",
    "    loss += svi.evaluate_loss(img.flatten(1).to(device), label.to(device))\n",
    "    return loss/(parameters[\"batch_size\"]**2)\n",
    "\n",
    "\n",
    "# ---------------------\n",
    "#  Loads Models Or Trains Classifier, Generator, Discriminator Models\n",
    "# ---------------------\n",
    "def load_or_train_models():\n",
    "    create_classifier_model()\n",
    "\n",
    "    if parameters[\"continue_on_existing_training\"] or (\n",
    "            not (os.path.exists('DiscriminatorModel.pt') or os.path.exists('GeneratorModel.pt'))):\n",
    "        sampler = SpecialSampler(mask, trainset)\n",
    "        special_loader = torch.utils.data.DataLoader(\n",
    "            trainset,\n",
    "            drop_last=True,\n",
    "            batch_size=parameters[\"batch_size\"],\n",
    "            sampler=sampler,\n",
    "            shuffle=False\n",
    "        )\n",
    "        if parameters[\"continue_on_existing_training\"] == True and (\n",
    "                os.path.exists('DiscriminatorModel.pt') and os.path.exists('GeneratorModel.pt')):\n",
    "            load_GAN_models()\n",
    "\n",
    "        train_GAN(special_loader)\n",
    "    else:\n",
    "        load_GAN_models()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------\n",
    "#  Generator Class\n",
    "# ---------------------\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        def block(in_feat, out_feat, normalize=True):\n",
    "            layers = [nn.Linear(in_feat, out_feat)]\n",
    "            if normalize:\n",
    "                layers.append(nn.BatchNorm1d(out_feat, 0.8))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "\n",
    "        self.fc = nn.Linear(parameters[\"latent_dim\"], parameters[\"channels\"] * parameters[\"img_size\"] ** 2)\n",
    "\n",
    "        self.init_size = parameters[\"img_size\"] // 4  # Initial size before upsampling\n",
    "\n",
    "        self.l1 = nn.Sequential(nn.Conv2d(parameters[\"channels\"] * 2, 64, 3, 1, 1), nn.ReLU(inplace=True))\n",
    "\n",
    "        self.conv_blocks_for_image = nn.Sequential(\n",
    "            nn.Conv2d(1, 1, 3, stride=2, padding=1),\n",
    "            nn.Dropout2d(0.1),\n",
    "            nn.BatchNorm2d(1, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "\n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout2d(0.25),\n",
    "            nn.Conv2d(128, 256, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout2d(0.5),\n",
    "            nn.Conv2d(256, 128, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout2d(0.5),\n",
    "            nn.Conv2d(128, 64, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout2d(0.5),\n",
    "            nn.Conv2d(64, 32, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(32, 16, 3, 1, 1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout2d(0.2),\n",
    "            nn.BatchNorm2d(16, 0.8),\n",
    "            nn.Conv2d(16, 1, 3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "        self.l2 = nn.Sequential(\n",
    "            nn.Linear(4 * parameters[\"img_size\"] ** 2, parameters[\"channels\"] * parameters[\"img_size\"] ** 2),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, z, img):\n",
    "        # img = play_with_image(img)\n",
    "        first_z = self.fc(z).view(\n",
    "            [parameters[\"batch_size\"], 1, int(parameters[\"img_size\"]), int(parameters[\"img_size\"])])\n",
    "        gen_input = torch.cat((FloatTensor(img), first_z), 1)\n",
    "        out = self.l1(gen_input)\n",
    "        generated_img = self.conv_blocks(out)\n",
    "        return generated_img\n",
    "\n",
    "\n",
    "# ---------------------\n",
    "#  Discriminator Class\n",
    "# ---------------------\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(int(np.prod(img_shape)) + parameters[\"n_classes\"], 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(256, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, img, labels):\n",
    "        gen_input = torch.cat(\n",
    "            (img.view(parameters[\"img_size\"] * parameters[\"img_size\"], parameters[\"batch_size\"]),\n",
    "             labels.view(parameters[\"n_classes\"], parameters[\"batch_size\"])), 0). \\\n",
    "            view(parameters[\"batch_size\"], parameters[\"img_size\"] * parameters[\"img_size\"] + parameters[\"n_classes\"])\n",
    "        validity = self.model(gen_input)\n",
    "        return validity\n",
    "\n",
    "\n",
    "generator = Generator()\n",
    "discriminator = Discriminator()\n",
    "\n",
    "if cuda:\n",
    "    generator.cuda()\n",
    "    discriminator.cuda()\n",
    "\n",
    "generator_optimizer = torch.optim.Adam(generator.parameters(), lr=parameters[\"lr_GAN\"],\n",
    "                                       betas=(parameters[\"b1\"], parameters[\"b2\"]))\n",
    "discriminator_optimizer = torch.optim.Adam(discriminator.parameters(), lr=parameters[\"lr_GAN\"],\n",
    "                                           betas=(parameters[\"b1\"], parameters[\"b2\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------\n",
    "#  Computes Gradient Penalty\n",
    "# ---------------------\n",
    "def compute_gradient_penalty(discriminator, real_samples, fake_samples, hot_labels):\n",
    "    alpha = FloatTensor(np.random.random((real_samples.size(0), 1, 1, 1)))\n",
    "    interpolates = (alpha * real_samples + ((1 - alpha) * fake_samples)).requires_grad_(True)\n",
    "    gradients = autograd.grad(\n",
    "        discriminator(interpolates, hot_labels),\n",
    "        interpolates,\n",
    "        Variable(FloatTensor(parameters[\"batch_size\"], 1).fill_(1.0), requires_grad=False),\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "        only_inputs=True,\n",
    "    )\n",
    "    gradients = gradients[0].view(gradients[0].size(0), -1)\n",
    "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "    return gradient_penalty\n",
    "\n",
    "# ---------------------\n",
    "#  Creates One Hot Label Representation\n",
    "# ---------------------\n",
    "def create_one_hot_label(labels):\n",
    "    hot_labels = []\n",
    "    hot_label = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "    for i in range(len(labels)):\n",
    "        hot_label[labels[i]] = 1\n",
    "        hot_labels.append(hot_label)\n",
    "\n",
    "    return FloatTensor(hot_labels)\n",
    "\n",
    "\n",
    "# ---------------------\n",
    "#  Calculates Discriminator Loss\n",
    "# ---------------------\n",
    "def calculate_discriminator_loss(real_images, real_labels, fake_images, hot_labels):\n",
    "    real_validity = discriminator(real_images, hot_labels)\n",
    "    fake_validity = discriminator(fake_images, hot_labels)\n",
    "\n",
    "    # Calculating gradient penalty\n",
    "    gradient_penalty = compute_gradient_penalty(discriminator, real_images.data, fake_images.data, hot_labels)\n",
    "    # Calculating transformation penalty\n",
    "    transformation_loss = calculate_transformation_loss(fake_images, real_labels)\n",
    "\n",
    "    # Calculating total penalty\n",
    "    total_loss = -torch.mean(real_validity) + \\\n",
    "                 torch.mean(fake_validity) + \\\n",
    "                 parameters[\"lambda_gp\"] * gradient_penalty + \\\n",
    "                 parameters[\"lambda_loss\"] * transformation_loss\n",
    "\n",
    "    return total_loss\n",
    "\n",
    "\n",
    "# ---------------------\n",
    "#  Prints GAN progress\n",
    "# ---------------------\n",
    "def print_progress(epoch, d_loss, g_loss):\n",
    "    print(\"[Epoch %d/%d] [D loss: %f] [G loss: %f]\"\n",
    "          % (epoch, parameters[\"n_epochs_GAN\"], d_loss.item(), g_loss.item())\n",
    "          )\n",
    "\n",
    "    \n",
    "# ---------------------\n",
    "#  Converts Images and Labels\n",
    "# ---------------------\n",
    "def arrange_data_tensors(images, labels):\n",
    "    images = Variable(images.type(FloatTensor))\n",
    "    labels = Variable(labels.type(LongTensor))\n",
    "\n",
    "    return images, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------------\n",
    "#  Loads Generator and Discriminator Models\n",
    "# ---------------------\n",
    "def load_GAN_models():\n",
    "    generator.load_state_dict(torch.load('GeneratorModel.pt'))\n",
    "    discriminator.load_state_dict(torch.load('DiscriminatorModel.pt'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------\n",
    "#  Generates image from generator and saves if save parameter is true\n",
    "# ---------------------\n",
    "def sample_single_image(images, index, image_path, save):\n",
    "    z = Variable(FloatTensor(np.random.normal(0, 1, (parameters[\"batch_size\"], parameters[\"latent_dim\"]))))\n",
    "    generated_images = generator(z, images)\n",
    "    if (save == True):\n",
    "        save_image(generated_images[index].data, image_path, nrow=1, normalize=True)\n",
    "\n",
    "    return generated_images[index].data\n",
    "\n",
    "\n",
    "# ---------------------\n",
    "#  Shows the given image\n",
    "# ---------------------\n",
    "def show_image(img):\n",
    "    img = img.view(parameters[\"img_size\"], parameters[\"img_size\"])\n",
    "    img = img.type(torch.FloatTensor).detach().numpy()\n",
    "    plt.imshow(img, cmap='gray')\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Models And Printing Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test_models produces quantitive and qualitive results.\n",
    "\n",
    "1)Quantitive result is generating a handwritten number which has a certainty that is under threshold.(Figure 2 from Paper)\n",
    "\n",
    "2)Qualitive result is printing three rows of the \"Table 1\" from the paper.\n",
    "It produces results for [0.7, 0.8, 0.9] thresholds (Three rows of the Table 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating images and printing results...\n",
      "-------------------Results for Threshold: 0.700000 -------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:66: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Qualitative Results for Threshold----------\n",
      "Original Image : 0.556490 (Prediction Acc) || Old Label:  3\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Image: 1.000000 (Prediction Acc) || New Label:  1\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Qualitative Results for Threshold----------\n",
      "\n",
      "----------Quantitative Results for Threshold----------\n",
      "Reject            : 0.066000\n",
      "BNN Accuracy      : 43.939394\n",
      "BNN+GWIN Accuracy : 62.121212\n",
      "Rejected Accuracy : 18\n",
      "Overall Accuracy  : 0.012000\n",
      "Error             : 27.548209\n",
      "----------Quantitative Results for Threshold----------\n",
      "-------------------Results for Threshold: 0.700000 -------------------\n",
      "\n",
      "\n",
      "-------------------Results for Threshold: 0.800000 -------------------\n",
      "----------Qualitative Results for Threshold----------\n",
      "Original Image : 0.714864 (Prediction Acc) || Old Label:  5\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Image: 1.000000 (Prediction Acc) || New Label:  9\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Qualitative Results for Threshold----------\n",
      "\n",
      "----------Quantitative Results for Threshold----------\n",
      "Reject            : 0.105000\n",
      "BNN Accuracy      : 40.952381\n",
      "BNN+GWIN Accuracy : 59.047619\n",
      "Rejected Accuracy : 18\n",
      "Overall Accuracy  : 0.019000\n",
      "Error             : 17.233560\n",
      "----------Quantitative Results for Threshold----------\n",
      "-------------------Results for Threshold: 0.800000 -------------------\n",
      "\n",
      "\n",
      "-------------------Results for Threshold: 0.900000 -------------------\n",
      "----------Qualitative Results for Threshold----------\n",
      "Original Image : 0.628692 (Prediction Acc) || Old Label:  4\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Image: 1.000000 (Prediction Acc) || New Label:  9\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Qualitative Results for Threshold----------\n",
      "\n",
      "----------Quantitative Results for Threshold----------\n",
      "Reject            : 0.166000\n",
      "BNN Accuracy      : 37.349398\n",
      "BNN+GWIN Accuracy : 60.843373\n",
      "Rejected Accuracy : 23\n",
      "Overall Accuracy  : 0.039000\n",
      "Error             : 14.152998\n",
      "----------Quantitative Results for Threshold----------\n",
      "-------------------Results for Threshold: 0.900000 -------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ---------------------\n",
    "#  Test\n",
    "# ---------------------\n",
    "def test_models():\n",
    "    load_or_train_models()\n",
    "    print(\"Generating images and printing results...\")\n",
    "    for threshold_index in range(len(parameters[\"threshold\"])):\n",
    "        print('-------------------Results for Threshold: %2f -------------------' % parameters[\"threshold\"][threshold_index])\n",
    "        counter = 0\n",
    "        probUnderThreshold = 0\n",
    "        probUnderThresholdButCorrectClassified = 0\n",
    "        probUnderThresholdThenClassifiedCorrectClassified = 0\n",
    "        probAboveThresholdAndCorrectClassified = 0\n",
    "        probAboveThresholdAlsoClassifiedCorrectClassified = 0\n",
    "\n",
    "        for i in range(10):\n",
    "            for j in range(int(len(testset) / parameters[\"batch_size\"])):\n",
    "                # obtain one batch of training images\n",
    "                dataiter = iter(testloader)\n",
    "                images, labels = dataiter.next()\n",
    "\n",
    "                images, labels = arrange_data_tensors(images, labels)\n",
    "\n",
    "                # get sample outputs\n",
    "                output = model(images)\n",
    "\n",
    "                predsProbs, preds = predict(images, labels)\n",
    "\n",
    "                # convert output probabilities to predicted class\n",
    "                # predsProbs, preds = torch.max(output, 1)\n",
    "\n",
    "                for k in range(parameters[\"batch_size\"]):\n",
    "\n",
    "                    idx = (i * int(len(testset))) + (j * parameters[\"batch_size\"]) + k\n",
    "\n",
    "                    trueLabel = labels[k].item()\n",
    "                    modelsPrediction = preds[k].item()\n",
    "                    modelsPredictionProb = predsProbs[k].item()\n",
    "\n",
    "                    if (modelsPredictionProb < parameters[\"threshold\"][threshold_index]):\n",
    "                        probUnderThreshold = probUnderThreshold + 1\n",
    "                        if (trueLabel == modelsPrediction):\n",
    "                            probUnderThresholdButCorrectClassified = probUnderThresholdButCorrectClassified + 1\n",
    "                    else:\n",
    "                        if (trueLabel == modelsPrediction):\n",
    "                            probAboveThresholdAndCorrectClassified = probAboveThresholdAndCorrectClassified + 1\n",
    "\n",
    "                    if (predsProbs[k] < parameters[\"threshold\"][threshold_index]):\n",
    "                        save_image(images[k], \"images/result_images/%d\" % idx + \"_original_image.png\", nrow=1,\n",
    "                                   normalize=True)\n",
    "\n",
    "                        generatedImage = sample_single_image(images, k,\n",
    "                                                             \"images/result_images/%d\" % idx + \"_generated_image_.png\",\n",
    "                                                             True)\n",
    "                        lastPredsProbs, lastPreds = predict(generatedImage.view(1, 1, 28, 28), labels)\n",
    "\n",
    "                        if not modelsPrediction == trueLabel and lastPreds.item() == trueLabel and counter < 1:\n",
    "                            print('----------Qualitative Results for Threshold----------')\n",
    "                            print('Original Image : %2f (Prediction Acc) || Old Label: %2d' % (predsProbs[k], preds[k]))\n",
    "                            show_image(images[k])\n",
    "                            print(\"Generated Image: %2f (Prediction Acc) || New Label: %2d\" % (lastPredsProbs.item(), lastPreds.item()))\n",
    "                            show_image(generatedImage)\n",
    "                            counter = counter + 1\n",
    "                            print('----------Qualitative Results for Threshold----------\\n')\n",
    "\n",
    "                        if (lastPreds.item() == trueLabel):\n",
    "                            probUnderThresholdThenClassifiedCorrectClassified = probUnderThresholdThenClassifiedCorrectClassified + 1\n",
    "\n",
    "        total_images = 100000.0\n",
    "\n",
    "        reject = (float(100 * probUnderThreshold) / total_images)\n",
    "        bnnAcc = (100 * probUnderThresholdButCorrectClassified / probUnderThreshold)\n",
    "        bnn_and_gwinAcc = (100 * probUnderThresholdThenClassifiedCorrectClassified / probUnderThreshold)\n",
    "        overallAcc = (float(100 * (\n",
    "                probUnderThresholdThenClassifiedCorrectClassified - probUnderThresholdButCorrectClassified)) / total_images)\n",
    "        error = (100 * (bnn_and_gwinAcc - bnnAcc)) / probUnderThreshold\n",
    "\n",
    "        print('----------Quantitative Results for Threshold----------')\n",
    "        print('Reject            : %2f' % reject)\n",
    "        print('BNN Accuracy      : %2f' % bnnAcc)\n",
    "        print('BNN+GWIN Accuracy : %2f' % bnn_and_gwinAcc)\n",
    "        print('Rejected Accuracy : %2d' % (bnn_and_gwinAcc - bnnAcc))\n",
    "        print('Overall Accuracy  : %2f' % overallAcc)\n",
    "        print('Error             : %2f' % error)\n",
    "        print('----------Quantitative Results for Threshold----------')\n",
    "\n",
    "        print('-------------------Results for Threshold: %2f -------------------\\n\\n' % parameters[\"threshold\"][threshold_index])\n",
    "\n",
    "        \n",
    "test_models()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
