{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paper Information\n",
    "    Name: Generative Well-intentioned Networks \n",
    "    Link: https://papers.nips.cc/paper/9467-generative-well-intentioned-networks.pdf\n",
    "    \n",
    "# Authors of Code\n",
    "    Yasin Berk Gültekin - 1942119 - \"e194211@metu.edu.tr\"\n",
    "    Hasan Ali Duran - 1941996 - \"e194199@metu.edu.tr\"\n",
    "\n",
    "# Paper Summary\n",
    "\n",
    "Paper proposes a new framework in order to increase the classifier’s accuracy rate. It uses conditional GAN to understand the distribution of the data which classifier labels with high certainty. It adds an rejection option to classifier since making a prediction about a uncertain input would decrease the accuracy. These rejected inputs translated by the GAN to high-certainty inputs and then relabeled by the classifier.\n",
    "They combined different loss functions in the paper:\n",
    "![LossFunction.PNG](https://github.com/berkgultekin/CGWIN/blob/master/LossFunction.PNG?raw=true)\n",
    " \n",
    "Paper uses Wasserstein GAN with gradient penalty and adds Transformation Penalty. Transformation Penalty is a new loss function with a transformation penalty that encourages the conditional generator to produce images that the classifier will label correctly. This penalty is the loss of the classifier when labeling the transformed observations in the current training batch\n",
    "The WGAN-GP critic is typically trained on both generated data and real data. However, they want the GWIN to generate images from the classifier’s confident distribution. Thus, they prefilter the training data to create a confident distribution Pc containing all images that the classifier labels correctly with high certainty. The critic is then trained exclusively on samples drawn from Pc and generated images by the generator.\n",
    "Paper uses two different datasets; the MNIST handwritten digits dataset and the Fashion-MNIST clothing dataset.\n",
    "\n",
    "# Implementation Steps\n",
    "\n",
    "First of all, we had to calculate a classification precision score for mnist data. For this reason, we had to first implement a classifier. We have implemented our Bayesian Neural Network classifier. Then we train the classifier using all the mnist data. Afterwards, our classifier has calculated the score for each data separately. We used these scores in order to create a subset of our data. And we called this subset critic dataset, we structured this critic data to consist only of those with very high classification scores.\n",
    "Then we wanted to recreate the data whose classification scores are below a certain threshold, through our generative model. To achieve this, we first implemented a vanillia gan. Later, we updated the discrimator to add contidition to vanillia gan and updated the images to use class label information as input. Then we updated Generator to receive an image as input because we wanted to produce the given image in a better quality. Later, we updated our GAN model by adding the loss functions mentioned in the paper. First of all, we made Wasserstein GAN. We then added a gradient penalty to develop the discriminator. Finally, we calculated the transformation loss and thus, we enabled our Generator to improve for the images we have generated. The model we have has turned into a conditionally customized version of Wasserstein Gan with Gradient Penalty. We then trained only the discriminator using the critic data, but the generator was trained using all the data.\n",
    "After the entire train process was over, we sent and recreated the data that the classifier did not give enough confidence score to our model. We then calculated the score using the classifier again for the re-created images and also classified them. Then, we compared the original results using the classification results of the new score values and the recreated images. Then, we presented these results to users for comparison.\n",
    "    \n",
    "    \n",
    "# Workflow of Code\n",
    "\n",
    "![CodeStructure.PNG](https://github.com/berkgultekin/CGWIN/blob/master/CodeStructure.PNG?raw=true)\n",
    "\n",
    "Our system first checks to see if we have a train model. If there is a model that has already been trained, it will load or load these models separately as Classifier and GAN models. If the train operation is to be performed again, it creates the critic dataset because the discriminator in Gan model becomes the train using this data. It also saves the related models after the train operation is finished.\n",
    "When an image comes next, this image is first examined in the classifier model and an attempt is made to estimate a class label, as well as a confidence score is calculated. If this score is below a certain threshold, the classifier rejects this image and the rejected image is sent to the GAN model. The GAN model uses a rejected image to produce a new image to preserve the image features. This expects the produced image to have a higher confidence score and to be correctly classified.\n",
    "Then, the re-created image is sent to the classifier and expected to classify again. Classifier's last results are compared with the first results and accordingly, it is calculated how our model offers an overall improvement.\n",
    "\n",
    "# Challenges Encountered When Implementing Paper\n",
    "\n",
    "There was not enough detail about how BNN was implemented. The number of layers was not specified. We could not fully obtain the BNN results mentioned in Paper. The BNN we have implemented makes predictions with higher scores. This situation caused difficulties in the exact occurrence of qualitative results.\n",
    "    \n",
    "The biggest problem we encountered during GAN implementation was that it was not clear enough how the Generator and Discriminator inputs should be processed in the model. It was not specified how many layers or what types of layers were used. In addition, the pictures produced by the generator appeared similar to those given as input to the Generator. There was no explanation for how this problem was solved in paper implementation. The new method which is transformation loss used during the Discriminator's loss calculation was not sufficiently explained. You can find our assumptions for the models(like number of layers and type of layers) in the implementation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-user install because site-packages writeable\n",
      "Created temporary directory: C:\\Users\\Lenovo\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-0ne_xsly\n",
      "Created temporary directory: C:\\Users\\Lenovo\\AppData\\Local\\Temp\\pip-req-tracker-1hpczs_w\n",
      "Initialized build tracking at C:\\Users\\Lenovo\\AppData\\Local\\Temp\\pip-req-tracker-1hpczs_w\n",
      "Created build tracker: C:\\Users\\Lenovo\\AppData\\Local\\Temp\\pip-req-tracker-1hpczs_w\n",
      "Entered build tracker: C:\\Users\\Lenovo\\AppData\\Local\\Temp\\pip-req-tracker-1hpczs_w\n",
      "Created temporary directory: C:\\Users\\Lenovo\\AppData\\Local\\Temp\\pip-install-0z0rzlzs\n",
      "Requirement already satisfied: pyro-ppl in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (1.3.1)\n",
      "Requirement already satisfied: numpy>=1.7 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from pyro-ppl) (1.18.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from pyro-ppl) (3.2.1)\n",
      "Requirement already satisfied: torch>=1.4.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from pyro-ppl) (1.5.0)\n",
      "Requirement already satisfied: pyro-api>=0.1.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from pyro-ppl) (0.1.2)\n",
      "Requirement already satisfied: tqdm>=4.36 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from pyro-ppl) (4.42.1)\n",
      "Requirement already satisfied: future in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from torch>=1.4.0->pyro-ppl) (0.18.2)\n",
      "Cleaning up...\n",
      "Removed build tracker: 'C:\\\\Users\\\\Lenovo\\\\AppData\\\\Local\\\\Temp\\\\pip-req-tracker-1hpczs_w'\n"
     ]
    }
   ],
   "source": [
    "!pip install -v pyro-ppl\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets\n",
    "import os.path\n",
    "import os\n",
    "import numpy as np\n",
    "from torchvision.utils import save_image\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.autograd as autograd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import subprocess\n",
    "import pyro.distributions as dist\n",
    "import pyro\n",
    "import plotly.graph_objects as go\n",
    "from matplotlib import colors\n",
    "from pyro import optim\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "\n",
    "import Datasets\n",
    "import Utils\n",
    "from Parameters import hyper_parameters as parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HyperParameters \n",
    "\n",
    "    \"threshold\": [0.7, 0.8, 0.9],\n",
    "    \"critic_threshold\": 0.95,\n",
    "    \"n_critic\": 5,\n",
    "    \"n_epochs_Classifier\": 30,\n",
    "    \"n_epochs_GAN\": 200000,\n",
    "    \"batch_size\": 128,\n",
    "    \"lr_Classifier\": 0.001,\n",
    "    \"lr_GAN\": 0.0001,\n",
    "    \"b1\": 0.5,\n",
    "    \"b2\": 0.9,\n",
    "    \"latent_dim\": 100,\n",
    "    \"n_classes\": 10,\n",
    "    \"img_size\": 28,\n",
    "    \"channels\": 1,\n",
    "    \"lambda_gp\": 10,\n",
    "    \"lambda_loss\": 10,\n",
    "    \"continue_on_existing_training\": 0,\n",
    "    \"run_download_sh\": 0\n",
    "    \n",
    "    These hyperparameters are set in the Parameters.py file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = True\n",
    "device = 'cuda' if cuda else 'cpu'\n",
    "Utils.set_device(cuda)\n",
    "\n",
    "FloatTensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if cuda else torch.LongTensor\n",
    "\n",
    "img_shape = (parameters[\"channels\"], parameters[\"img_size\"], parameters[\"img_size\"])\n",
    "\n",
    "# ---------------------\n",
    "#  Running Download Script\n",
    "# ---------------------\n",
    "if parameters[\"run_download_sh\"]:\n",
    "    subprocess.call(\"download.sh\", shell=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------\n",
    "#  BNN - Classifier Model\n",
    "# ---------------------\n",
    "class BNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BNN, self).__init__()\n",
    "\n",
    "        self.out = nn.Sequential(\n",
    "            nn.Linear(parameters[\"img_size\"] ** 2, 1024),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(1024, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.flatten(1)\n",
    "        return self.out(x)\n",
    "\n",
    "\n",
    "# ---------------------\n",
    "#  Generator Class\n",
    "# ---------------------\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        def block(in_feat, out_feat, normalize=True):\n",
    "            layers = [nn.Linear(in_feat, out_feat)]\n",
    "            if normalize:\n",
    "                layers.append(nn.BatchNorm1d(out_feat, 0.8))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "\n",
    "        self.fc = nn.Linear(parameters[\"latent_dim\"], parameters[\"channels\"] * parameters[\"img_size\"] ** 2)\n",
    "\n",
    "        self.l1 = nn.Sequential(nn.Conv2d(parameters[\"channels\"] * 2, 64, 3, 1, 1), nn.ReLU(inplace=True))\n",
    "\n",
    "        self.conv_blocks_for_image = nn.Sequential(\n",
    "            nn.Conv2d(1, 1, 3, stride=2, padding=1),\n",
    "            nn.Dropout2d(0.1),\n",
    "            nn.BatchNorm2d(1, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "\n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout2d(0.25),\n",
    "            nn.Conv2d(128, 256, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout2d(0.5),\n",
    "            nn.Conv2d(256, 128, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout2d(0.5),\n",
    "            nn.Conv2d(128, 64, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout2d(0.5),\n",
    "            nn.Conv2d(64, 32, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(32, 16, 3, 1, 1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout2d(0.2),\n",
    "            nn.BatchNorm2d(16, 0.8),\n",
    "            nn.Conv2d(16, 1, 3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "        self.l2 = nn.Sequential(\n",
    "            nn.Linear(4 * parameters[\"img_size\"] ** 2, parameters[\"channels\"] * parameters[\"img_size\"] ** 2),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, z, img):\n",
    "        # img = play_with_image(img)\n",
    "        first_z = self.fc(z).view(\n",
    "            [parameters[\"batch_size\"], 1, int(parameters[\"img_size\"]), int(parameters[\"img_size\"])])\n",
    "        gen_input = torch.cat((img, first_z), 1)\n",
    "        out = self.l1(gen_input)\n",
    "        generated_img = self.conv_blocks(out)\n",
    "        return generated_img\n",
    "\n",
    "\n",
    "# ---------------------\n",
    "#  Discriminator Class\n",
    "# ---------------------\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(int(np.prod(img_shape)) + parameters[\"n_classes\"], 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(256, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, img, labels):\n",
    "        gen_input = torch.cat(\n",
    "            (img.view(parameters[\"img_size\"] * parameters[\"img_size\"], parameters[\"batch_size\"]),\n",
    "             labels.view(parameters[\"n_classes\"], parameters[\"batch_size\"])), 0). \\\n",
    "            view(parameters[\"batch_size\"], parameters[\"img_size\"] * parameters[\"img_size\"] + parameters[\"n_classes\"])\n",
    "        validity = self.model(gen_input)\n",
    "        return validity\n",
    "    \n",
    "generator = Generator()\n",
    "discriminator = Discriminator()\n",
    "\n",
    "if cuda:\n",
    "    generator.cuda()\n",
    "    discriminator.cuda()\n",
    "\n",
    "generator_optimizer = torch.optim.Adam(generator.parameters(), lr=parameters[\"lr_GAN\"],\n",
    "                                       betas=(parameters[\"b1\"], parameters[\"b2\"]))\n",
    "discriminator_optimizer = torch.optim.Adam(discriminator.parameters(), lr=parameters[\"lr_GAN\"],\n",
    "                                           betas=(parameters[\"b1\"], parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier Model (Bayesian Neural Network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BNN()\n",
    "if cuda:\n",
    "    model.cuda()\n",
    "\n",
    "\n",
    "# ---------------------\n",
    "#  Stochastic Variational Inference's Module\n",
    "# ---------------------\n",
    "def module(x, y):\n",
    "    priors = {}\n",
    "    for iterator in model.named_parameters():\n",
    "        name, param = iterator\n",
    "        zeros = torch.zeros_like(param.data)\n",
    "        ones = torch.ones_like(param.data)\n",
    "        priors[name] = dist.Normal(loc=zeros,\n",
    "                                   scale=ones)\n",
    "\n",
    "    lifted_module = pyro.random_module(\"module\", model, priors)\n",
    "    lifted_module_method = lifted_module()\n",
    "    lhat = F.log_softmax(lifted_module_method(x), 1)\n",
    "    pyro.sample(\"obs\", dist.Categorical(logits=lhat), obs=y)\n",
    "\n",
    "\n",
    "# ---------------------\n",
    "#  Stochastic Variational Inference's Guide\n",
    "# ---------------------\n",
    "def guide(x, y):\n",
    "    priors = {}\n",
    "    for iterator in model.named_parameters():\n",
    "        name, param = iterator\n",
    "        priors[name] = dist.Normal(loc=pyro.param(name + '.mu', torch.randn_like(param)),\n",
    "                                   scale=F.softplus(pyro.param(name + '.sigma', torch.randn_like(param))))\n",
    "\n",
    "    lifted_module = pyro.random_module('module', model, priors)\n",
    "    return lifted_module()\n",
    "\n",
    "opt = optim.Adam({'lr': parameters[\"lr_Classifier\"]})\n",
    "svi = SVI(module, guide, opt, loss=Trace_ELBO())\n",
    "\n",
    "# ---------------------\n",
    "#  Classifier's prediction method\n",
    "# ---------------------\n",
    "def predict(x, y):\n",
    "    sampled_models = [guide(None, None) for _ in range(parameters[\"n_classes\"])]\n",
    "    yhats = [model(x.to(device)).data for model in sampled_models]\n",
    "    mean = torch.mean(torch.stack(yhats), 0)\n",
    "    predsProbs, preds = torch.max(F.softmax(mean).to(device), 1)\n",
    "    return predsProbs, preds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Masked Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating mask to train model with only critic dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\pyro\\primitives.py:406: FutureWarning: The `random_module` primitive is deprecated, and will be removed in a future release. Use `pyro.nn.Module` to create Bayesian modules from `torch.nn.Module` instances.\n",
      "  \"modules from `torch.nn.Module` instances.\", FutureWarning)\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "mask = []\n",
    "\n",
    "# ---------------------\n",
    "#  Creates Mask To Train Discriminator With P_Critic\n",
    "# ---------------------\n",
    "def create_critic_mask():\n",
    "    print(\"Creating mask to train model with only critic dataset...\")\n",
    "    for i in range(int(len(Datasets.trainset) / parameters[\"batch_size\"])):\n",
    "        # obtain one batch of training images\n",
    "        dataiter = iter(Datasets.trainloader)\n",
    "        images, labels = dataiter.next()\n",
    "        images, labels = Utils.arrange_data_tensors(images, labels)\n",
    "\n",
    "        predsProbs, preds= predict(images, labels)\n",
    "        # convert output probabilities to predicted class\n",
    "        # predsProbs, preds = torch.max(output, 1)\n",
    "        for j in range(parameters[\"batch_size\"]):\n",
    "            mask.append(1 if predsProbs[j].item() > parameters[\"critic_threshold\"] else 0)\n",
    "\n",
    "\n",
    "create_critic_mask()\n",
    "mask = FloatTensor(mask)\n",
    "\n",
    "\n",
    "# ---------------------\n",
    "#  Sampler Class to use mask\n",
    "# ---------------------\n",
    "class SpecialSampler(torch.utils.data.sampler.Sampler):\n",
    "    def __init__(self, mask, data_source):\n",
    "        self.mask = mask\n",
    "        self.data_source = data_source\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter([i.item() for i in torch.nonzero(mask)])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training And Saving Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ---------------------\n",
    "#  Trains Classifier Model\n",
    "# ---------------------\n",
    "\n",
    "def train_classifier_model():\n",
    "    pyro.clear_param_store()\n",
    "\n",
    "    total_loss = 0\n",
    "    for epoch in range(parameters[\"n_epochs_Classifier\"]):\n",
    "        loss = 0\n",
    "        for x, y in Datasets.trainloader:\n",
    "            loss += svi.step(x.flatten(1).to(device), y.to(device))\n",
    "        total_loss = loss / len(Datasets.trainloader.dataset)\n",
    "        print(\"Epoch: %d Loss: %f\" % (epoch + 1, total_loss))\n",
    "\n",
    "    pyro.get_param_store().save('paramstore.out')\n",
    "    torch.save(model.state_dict(), 'ClassifierModel.pt')\n",
    "\n",
    "# ---------------------\n",
    "#  Trains Generative Adverserial Network model\n",
    "# ---------------------\n",
    "def train_GAN():\n",
    "    sampler = SpecialSampler(mask, Datasets.trainset)\n",
    "    criticLoader = torch.utils.data.DataLoader(\n",
    "        Datasets.trainset,\n",
    "        drop_last=True,\n",
    "        batch_size=parameters[\"batch_size\"],\n",
    "        sampler=sampler,\n",
    "        shuffle=False\n",
    "    )\n",
    "    batches_done = 0\n",
    "    for epoch in range(parameters[\"n_epochs_GAN\"]):\n",
    "        for i, (imgs, labels) in enumerate(criticLoader):\n",
    "            hot_labels = Utils.create_one_hot_label(labels)\n",
    "\n",
    "            real_images = Variable(imgs.type(FloatTensor))\n",
    "            labels = Variable(labels.type(LongTensor))\n",
    "\n",
    "            discriminator_optimizer.zero_grad()\n",
    "\n",
    "            z = Variable(FloatTensor(np.random.normal(0, 1, (parameters[\"batch_size\"], parameters[\"latent_dim\"]))))\n",
    "\n",
    "            generated_images = generator(z, real_images)\n",
    "\n",
    "            d_loss = Utils.calculate_discriminator_loss(discriminator, real_images, generated_images, hot_labels)\n",
    "\n",
    "            d_loss.backward()\n",
    "            discriminator_optimizer.step()\n",
    "\n",
    "            generator_optimizer.zero_grad()\n",
    "\n",
    "            if i % parameters[\"n_critic\"] == 0:\n",
    "                dataiter = iter(Datasets.trainloader)\n",
    "                pr_images, pr_labels = dataiter.next()\n",
    "\n",
    "                pr_images = Variable(pr_images.type(FloatTensor))\n",
    "                pr_labels = Variable(pr_labels.type(LongTensor))\n",
    "\n",
    "                generated_images = generator(z, pr_images)\n",
    "                pr_hot_labels = Utils.create_one_hot_label(pr_labels)\n",
    "\n",
    "                fake_validity = discriminator(generated_images, pr_hot_labels)\n",
    "                # Calculating transformation penalty\n",
    "                transformation_loss = Utils.calculate_transformation_loss(generated_images, svi, pr_labels)\n",
    "\n",
    "                g_loss = -torch.mean(fake_validity) + (parameters[\"lambda_loss\"] * transformation_loss)\n",
    "\n",
    "                g_loss.backward()\n",
    "                generator_optimizer.step()\n",
    "\n",
    "                Utils.print_batch_progress(i, d_loss, g_loss)\n",
    "                if batches_done % 100 == 0:\n",
    "                    save_image(generated_images.data[:25], \"images/wgan/%d.png\" % batches_done, nrow=5, normalize=True)\n",
    "\n",
    "                batches_done += batches_done + parameters[\"n_critic\"]\n",
    "\n",
    "        Utils.print_epoch_progress(epoch + 1, d_loss, g_loss)\n",
    "\n",
    "    save_GAN_models()\n",
    "    \n",
    "# ---------------------\n",
    "#  Saves Generator and Discriminator Models\n",
    "# ---------------------\n",
    "def save_GAN_models():\n",
    "    torch.save(generator.state_dict(), 'GeneratorModel.pt')\n",
    "    torch.save(discriminator.state_dict(), 'DiscriminatorModel.pt')\n",
    "   \n",
    "    \n",
    "train_classifier_model()\n",
    "train_GAN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------\n",
    "#  Test for Classifier\n",
    "# ---------------------\n",
    "def test_classifier():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for x, y in Datasets.testloader:\n",
    "        x, y = Utils.arrange_data_tensors(x, y)\n",
    "        predsProbs, preds = predict(x.flatten(1), y)\n",
    "        total += parameters[\"batch_size\"]\n",
    "        correct += (preds == y).sum().item()\n",
    "    print(\"Accuracy: %f\" % (correct / total))\n",
    "\n",
    "test_classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------\n",
    "#  Loads Generator and Discriminator Models\n",
    "# ---------------------\n",
    "def load_GAN_models():\n",
    "    generator.load_state_dict(torch.load('GeneratorModel.pt' ,map_location=device))\n",
    "    discriminator.load_state_dict(torch.load('DiscriminatorModel.pt', map_location=device))\n",
    "    \n",
    "# ---------------------\n",
    "#  Trains or loads classifier model\n",
    "# ---------------------\n",
    "def create_classifier_model():\n",
    "    if (not os.path.exists('ClassifierModel.pt') or not os.path.exists('paramstore.out')):\n",
    "        train_classifier_model()\n",
    "        test_classifier()\n",
    "    else:\n",
    "        pyro.get_param_store().load('paramstore.out', map_location=device)\n",
    "        model.load_state_dict(torch.load('ClassifierModel.pt', map_location=device))\n",
    "\n",
    "# ---------------------\n",
    "#  Loads Models Or Trains Classifier, Generator, Discriminator Models\n",
    "# ---------------------\n",
    "def load_or_train_models():\n",
    "    create_classifier_model()\n",
    "\n",
    "    if parameters[\"continue_on_existing_training\"] or (\n",
    "            not (os.path.exists('DiscriminatorModel.pt') or os.path.exists('GeneratorModel.pt'))):\n",
    "        \n",
    "        if parameters[\"continue_on_existing_training\"] == True and (\n",
    "                os.path.exists('DiscriminatorModel.pt') and os.path.exists('GeneratorModel.pt')):\n",
    "            load_GAN_models()\n",
    "\n",
    "        train_GAN()\n",
    "    else:\n",
    "        load_GAN_models()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Models And Printing Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test_models produces quantitive and qualitive results.\n",
    "\n",
    "1)Quantitive result is generating a handwritten number which has a certainty that is under threshold.(Figure 2 from Paper)\n",
    "\n",
    "2)Qualitive result is printing three rows of the \"Table 1\" from the paper.\n",
    "It produces results for [0.7, 0.8, 0.9] thresholds (Three rows of the Table 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating images and printing results...\n"
     ]
    },
    {
     "data": {
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on dataset with Threshold: 0.700000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\pyro\\primitives.py:406: FutureWarning:\n",
      "\n",
      "The `random_module` primitive is deprecated, and will be removed in a future release. Use `pyro.nn.Module` to create Bayesian modules from `torch.nn.Module` instances.\n",
      "\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:47: UserWarning:\n",
      "\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ---------------------\n",
    "#  Test\n",
    "# ---------------------\n",
    "def test_models():\n",
    "    load_or_train_models()\n",
    "    print(\"Generating images and printing results...\")\n",
    "\n",
    "    tableHeader = dict(values=['τ', '%Reject', 'BNN Acc.', 'BNN+GWIN Acc.', 'Rejected Acc.', 'Overall Acc.', '% Error'])\n",
    "    tableCells = dict(values=[[1], [1], [1], [1], [1], [1], [1]])\n",
    "    a = go.Figure(data=[go.Table(header=tableHeader, cells=tableCells)])\n",
    "    a.show(\"png\")\n",
    "    \n",
    "    thresholds = []\n",
    "    rejects = []\n",
    "    bnnAccs = []\n",
    "    bnnGwınAccs = []\n",
    "    rejectedAccs = []\n",
    "    overallAccs = []\n",
    "    errors = []\n",
    "    showed_images = []\n",
    "    showed_texts = []\n",
    "\n",
    "    realValues = [[\"0.7 (Expected)\", \"0.8 (Expected)\", \"0.9 (Expected)\"],\n",
    "                  [1.83, 2.74, 4.39], [54.49, 58.91, 68.79],\n",
    "                  [85.07, 86.30, 86.95], [30.59, 27.39, 18.16],\n",
    "                  [0.56, 0.75, 0.80], [-27.55, -36.36, 40.26]]\n",
    "\n",
    "    for threshold_index in range(len(parameters[\"threshold\"])):\n",
    "        print('Working on dataset with Threshold: %2f' % round(parameters[\"threshold\"][threshold_index], 2))\n",
    "        counter = 0\n",
    "        probUnderThreshold = 0\n",
    "        probUnderThresholdButCorrectClassified = 0\n",
    "        probUnderThresholdThenClassifiedCorrectClassified = 0\n",
    "        probAboveThresholdAndCorrectClassified = 0\n",
    "        probAboveThresholdAlsoClassifiedCorrectClassified = 0\n",
    "\n",
    "        for i in range(10):\n",
    "            for j in range(int(len(Datasets.testset) / parameters[\"batch_size\"])):\n",
    "                # obtain one batch of training images\n",
    "                dataiter = iter(Datasets.testloader)\n",
    "                images, labels = dataiter.next()\n",
    "\n",
    "                images, labels = Utils.arrange_data_tensors(images, labels)\n",
    "\n",
    "                predsProbs, preds = predict(images, labels)\n",
    "\n",
    "                for k in range(parameters[\"batch_size\"]):\n",
    "\n",
    "                    idx = (i * int(len(Datasets.testset))) + (j * parameters[\"batch_size\"]) + k\n",
    "\n",
    "                    trueLabel = labels[k].item()\n",
    "                    modelsPrediction = preds[k].item()\n",
    "                    modelsPredictionProb = predsProbs[k].item()\n",
    "\n",
    "                    if (modelsPredictionProb < parameters[\"threshold\"][threshold_index]):\n",
    "                        probUnderThreshold = probUnderThreshold + 1\n",
    "                        if (trueLabel == modelsPrediction):\n",
    "                            probUnderThresholdButCorrectClassified = probUnderThresholdButCorrectClassified + 1\n",
    "                    else:\n",
    "                        if (trueLabel == modelsPrediction):\n",
    "                            probAboveThresholdAndCorrectClassified = probAboveThresholdAndCorrectClassified + 1\n",
    "\n",
    "                    if (predsProbs[k] < parameters[\"threshold\"][threshold_index]):\n",
    "                        save_image(images[k], \"images/result_images/%d\" % idx + \"_original_image.png\", nrow=1,\n",
    "                                   normalize=True)\n",
    "\n",
    "                        generatedImage = Utils.sample_single_image(images, k, generator,\n",
    "                                                                   \"images/result_images/%d\" % idx + \"_generated_image_.png\",\n",
    "                                                                   True)\n",
    "                        lastPredsProbs, lastPreds = predict(generatedImage.view(1, 1, 28, 28), labels)\n",
    "\n",
    "                        if not modelsPrediction == trueLabel and lastPreds.item() == trueLabel and counter < 5:\n",
    "                            showed_images.append(images[k])\n",
    "                            showed_images.append(generatedImage)\n",
    "                            showed_texts.append(\"Old Label: %2d\" %int(preds[k]) + \"(Prob: %2f\" %round(float(predsProbs[k]), 2) + \")\" + \" - \" +\n",
    "                                                \"New Label: %2d\" %int(lastPreds.item()) + \"(Prob: %2f\" %round(float(lastPredsProbs.item()), 2) + \")\")\n",
    "                            counter = counter + 1\n",
    "\n",
    "\n",
    "                        if (lastPreds.item() == trueLabel):\n",
    "                            probUnderThresholdThenClassifiedCorrectClassified = probUnderThresholdThenClassifiedCorrectClassified + 1\n",
    "\n",
    "        total_images = 100000.0\n",
    "        o = 0\n",
    "        m = 0\n",
    "        while o < len(showed_images):\n",
    "            Utils.showGrid(showed_images[o], showed_images[o + 1], showed_texts[m])\n",
    "            o = o + 2\n",
    "            m = m + 1\n",
    "\n",
    "        reject = (float(100 * probUnderThreshold) / total_images)\n",
    "        bnnAcc = (100 * probUnderThresholdButCorrectClassified / probUnderThreshold)\n",
    "        bnn_and_gwinAcc = (100 * probUnderThresholdThenClassifiedCorrectClassified / probUnderThreshold)\n",
    "        overallAcc = (float(100 * (\n",
    "                probUnderThresholdThenClassifiedCorrectClassified - probUnderThresholdButCorrectClassified)) / total_images)\n",
    "        error = (100 * (bnnAcc - bnn_and_gwinAcc)) / probUnderThreshold\n",
    "\n",
    "        thresholds.append(parameters[\"threshold\"][threshold_index])\n",
    "        thresholds.append(realValues[0][threshold_index])\n",
    "        rejects.append(reject)\n",
    "        rejects.append(realValues[1][threshold_index])\n",
    "        bnnAccs.append(round(bnnAcc, 2))\n",
    "        bnnAccs.append(realValues[2][threshold_index])\n",
    "        bnnGwınAccs.append(round(bnn_and_gwinAcc, 2))\n",
    "        bnnGwınAccs.append(realValues[3][threshold_index])\n",
    "        rejectedAccs.append(round((bnn_and_gwinAcc - bnnAcc), 2))\n",
    "        rejectedAccs.append(realValues[4][threshold_index])\n",
    "        overallAccs.append(overallAcc)\n",
    "        overallAccs.append(realValues[5][threshold_index])\n",
    "        errors.append(round(error, 2))\n",
    "        errors.append(realValues[6][threshold_index])\n",
    "\n",
    "    tableCells = dict(values=[thresholds, rejects, bnnAccs, bnnGwınAccs, rejectedAccs, overallAccs, errors])\n",
    "    resultsFig = go.Figure(data=[go.Table(header=tableHeader, cells=tableCells)])\n",
    "    resultsFig.show(\"png\")\n",
    "\n",
    "        \n",
    "test_models()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
