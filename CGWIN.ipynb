{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paper Information :\n",
    "    Generative Well-intentioned Networks \n",
    "    https://papers.nips.cc/paper/9467-generative-well-intentioned-networks.pdf\n",
    "\n",
    "Authors of code:\n",
    "    Yasin Berk GÃ¼ltekin - 1942119\n",
    "    Hasan Ali Duran - 1942119\n",
    "    \n",
    "    \n",
    "Challenges Encountered When Implementing Paper:\n",
    "* There was not enough detail about how BNN was implemented. The number of layers was not specified. We could not fully obtain the BNN results mentioned in Paper. The BNN we have implemented makes predictions with higher scores. This situation caused difficulties in the exact occurrence of qualitative results.\n",
    "\n",
    "* The biggest problem we encountered during GAN implementation was that it was not clear enough how the Generator and Discriminator inputs should be processed in the model. It was not specified how many layers or what types of layers were used. In addition, the pictures produced by the generator appeared similar to those given as input to the Generator. There was no explanation for how this problem was solved in paper implementation. The new method which is transformation loss used during the Discriminator's loss calculation was not sufficiently explained.(You can find our assumptions for the models(like number of layers and type of layers) in the implementation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-user install because site-packages writeable\n",
      "Created temporary directory: C:\\Users\\Lenovo\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-x0ausrdi\n",
      "Created temporary directory: C:\\Users\\Lenovo\\AppData\\Local\\Temp\\pip-req-tracker-ntczw9ee\n",
      "Initialized build tracking at C:\\Users\\Lenovo\\AppData\\Local\\Temp\\pip-req-tracker-ntczw9ee\n",
      "Created build tracker: C:\\Users\\Lenovo\\AppData\\Local\\Temp\\pip-req-tracker-ntczw9ee\n",
      "Entered build tracker: C:\\Users\\Lenovo\\AppData\\Local\\Temp\\pip-req-tracker-ntczw9ee\n",
      "Created temporary directory: C:\\Users\\Lenovo\\AppData\\Local\\Temp\\pip-install-vmsntved\n",
      "Requirement already satisfied: pyro-ppl in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (1.3.1)\n",
      "Requirement already satisfied: pyro-api>=0.1.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from pyro-ppl) (0.1.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from pyro-ppl) (3.2.1)\n",
      "Requirement already satisfied: tqdm>=4.36 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from pyro-ppl) (4.42.1)\n",
      "Requirement already satisfied: torch>=1.4.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from pyro-ppl) (1.5.0)\n",
      "Requirement already satisfied: numpy>=1.7 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from pyro-ppl) (1.18.1)\n",
      "Requirement already satisfied: future in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from torch>=1.4.0->pyro-ppl) (0.18.2)\n",
      "Cleaning up...\n",
      "Removed build tracker: 'C:\\\\Users\\\\Lenovo\\\\AppData\\\\Local\\\\Temp\\\\pip-req-tracker-ntczw9ee'\n"
     ]
    }
   ],
   "source": [
    "!pip install -v pyro-ppl\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets\n",
    "import os.path\n",
    "import os\n",
    "import numpy as np\n",
    "from torchvision.utils import save_image\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.autograd as autograd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import subprocess\n",
    "import pyro.distributions as dist\n",
    "import pyro\n",
    "from matplotlib import colors\n",
    "from pyro import optim\n",
    "from pyro.infer import SVI, Trace_ELBO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HyperParameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    \"threshold\": [0.7, 0.8, 0.9],\n",
    "    \"critic_threshold\": 0.95,\n",
    "    \"n_critic\": 5,\n",
    "    \"n_epochs_Classifier\": 30,\n",
    "    \"n_epochs_GAN\": 200000,\n",
    "    \"batch_size\": 128,\n",
    "    \"lr_Classifier\": 0.001, # learning rate for Classifier model\n",
    "    \"lr_GAN\": 0.0001,  # learning rate for GAN models\n",
    "    \"b1\": 0.5,\n",
    "    \"b2\": 0.9,\n",
    "    \"latent_dim\": 100,\n",
    "    \"n_classes\": 10,\n",
    "    \"img_size\": 28,\n",
    "    \"channels\": 1,\n",
    "    \"lambda_gp\": 10, # lambda for gradient penalty\n",
    "    \"lambda_loss\": 10, # lambda for transformation penalty\n",
    "    \"continue_on_existing_training\": 0,\n",
    "    \"cuda\": 1,\n",
    "    \"run_download_sh\": 0\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_shape = (parameters[\"channels\"], parameters[\"img_size\"], parameters[\"img_size\"])\n",
    "\n",
    "cuda = True if parameters[\"cuda\"] else False\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "FloatTensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if cuda else torch.LongTensor\n",
    "\n",
    "# ---------------------\n",
    "#  Setting Datasets\n",
    "# ---------------------\n",
    "if parameters[\"run_download_sh\"]:\n",
    "    subprocess.call(\"download.sh\", shell=True)\n",
    "\n",
    "trainset = datasets.MNIST(\n",
    "    root=\"\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.Compose(\n",
    "        [transforms.Resize(parameters[\"img_size\"]), transforms.ToTensor(), transforms.Normalize([0.5], [0.5])]))\n",
    "\n",
    "testset = datasets.MNIST(\n",
    "    root=\"\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transforms.Compose(\n",
    "        [transforms.Resize(parameters[\"img_size\"]), transforms.ToTensor(), transforms.Normalize([0.5], [0.5])]))\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset,\n",
    "    drop_last=True,\n",
    "    batch_size=parameters[\"batch_size\"],\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset,\n",
    "    drop_last=True,\n",
    "    batch_size=parameters[\"batch_size\"],\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classifier Model (Bayesian Neural Network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------\n",
    "#  BNN - Classifier Model\n",
    "# ---------------------\n",
    "class BNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BNN, self).__init__()\n",
    "\n",
    "        self.out = nn.Sequential(\n",
    "            nn.Linear(parameters[\"img_size\"] ** 2, 1024),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(1024, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.flatten(1)\n",
    "        return self.out(x)\n",
    "\n",
    "\n",
    "model = BNN()\n",
    "if cuda:\n",
    "    model.cuda()\n",
    "\n",
    "\n",
    "# ---------------------\n",
    "#  Stochastic Variational Inference's Module\n",
    "# ---------------------\n",
    "def module(x, y):\n",
    "    priors = {}\n",
    "    for iterator in model.named_parameters():\n",
    "        name, param = iterator\n",
    "        zeros = torch.zeros_like(param.data)\n",
    "        ones = torch.ones_like(param.data)\n",
    "        priors[name] = dist.Normal(loc=zeros,\n",
    "                                   scale=ones)\n",
    "\n",
    "    lifted_module = pyro.random_module(\"module\", model, priors)\n",
    "    lifted_module_method = lifted_module()\n",
    "    lhat = F.log_softmax(lifted_module_method(x), 1)\n",
    "    pyro.sample(\"obs\", dist.Categorical(logits=lhat), obs=y)\n",
    "\n",
    "\n",
    "# ---------------------\n",
    "#  Stochastic Variational Inference's Guide\n",
    "# ---------------------\n",
    "def guide(x, y):\n",
    "    priors = {}\n",
    "    for iterator in model.named_parameters():\n",
    "        name, param = iterator\n",
    "        priors[name] = dist.Normal(loc=pyro.param(name + '.mu', torch.randn_like(param)),\n",
    "                                   scale=F.softplus(pyro.param(name + '.sigma', torch.randn_like(param))))\n",
    "\n",
    "    lifted_module = pyro.random_module('module', model, priors)\n",
    "    return lifted_module()\n",
    "\n",
    "\n",
    "opt = optim.Adam({'lr': parameters[\"lr_Classifier\"]})\n",
    "svi = SVI(module, guide, opt, loss=Trace_ELBO())\n",
    "\n",
    "# ---------------------\n",
    "#  Classifier's prediction method\n",
    "# ---------------------\n",
    "def predict(x, y):\n",
    "    sampled_models = [guide(None, None) for _ in range(parameters[\"n_classes\"])]\n",
    "    yhats = [model(x.to(device)).data for model in sampled_models]\n",
    "    mean = torch.mean(torch.stack(yhats), 0)\n",
    "    predsProbs, preds = torch.max(F.softmax(mean).to(device), 1)\n",
    "    return predsProbs, preds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training And Saving Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Loss: 6414.542196\n",
      "Epoch: 2 Loss: 5750.218853\n",
      "Epoch: 3 Loss: 5376.938865\n"
     ]
    }
   ],
   "source": [
    "# ---------------------\n",
    "#  Trains Classifier Model\n",
    "# ---------------------\n",
    "def train_classifier_model():\n",
    "    pyro.clear_param_store()\n",
    "\n",
    "    total_loss = 0\n",
    "    for epoch in range(parameters[\"n_epochs_Classifier\"]):\n",
    "        loss = 0\n",
    "        for x, y in trainloader:\n",
    "            loss += svi.step(x.flatten(1).to(device), y.to(device))\n",
    "        total_loss = loss / len(trainloader.dataset)\n",
    "        print(\"Epoch: %d Loss: %f\" % (epoch + 1, total_loss))\n",
    "\n",
    "    pyro.get_param_store().save('paramstore.out')\n",
    "    torch.save(model.state_dict(), 'ClassifierModel.pt')\n",
    "\n",
    "train_classifier_model()\n",
    "\n",
    "# ---------------------\n",
    "#  Trains Generative Adverserial Network model\n",
    "# ---------------------\n",
    "def train_GAN(dataloader):\n",
    "    batches_done = 0\n",
    "    for epoch in range(parameters[\"n_epochs_GAN\"]):\n",
    "        for i, (imgs, labels) in enumerate(dataloader):\n",
    "            hot_labels = create_one_hot_label(labels)\n",
    "\n",
    "            real_images = Variable(imgs.type(FloatTensor))\n",
    "            labels = Variable(labels.type(LongTensor))\n",
    "\n",
    "            discriminator_optimizer.zero_grad()\n",
    "\n",
    "            z = Variable(FloatTensor(np.random.normal(0, 1, (parameters[\"batch_size\"], parameters[\"latent_dim\"]))))\n",
    "\n",
    "            generated_images = generator(z, real_images)\n",
    "\n",
    "            d_loss = calculate_discriminator_loss(real_images, labels, generated_images, hot_labels)\n",
    "\n",
    "            d_loss.backward()\n",
    "            discriminator_optimizer.step()\n",
    "\n",
    "            generator_optimizer.zero_grad()\n",
    "\n",
    "\n",
    "            if i % parameters[\"n_critic\"] == 0:\n",
    "\n",
    "                generated_images = generator(z, real_images)\n",
    "\n",
    "                fake_validity = discriminator(generated_images, hot_labels)\n",
    "                g_loss = -torch.mean(fake_validity)\n",
    "\n",
    "                g_loss.backward()\n",
    "                generator_optimizer.step()\n",
    "\n",
    "                print_progress(epoch, d_loss, g_loss)\n",
    "\n",
    "                batches_done += batches_done + parameters[\"n_critic\"]\n",
    "\n",
    "                if batches_done % 100 == 0:\n",
    "                    save_image(generated_images.data[:25], \"images/wgan/%d.png\" % batches_done, nrow=5, normalize=True)\n",
    "\n",
    "    save_GAN_models()\n",
    "    \n",
    "# ---------------------\n",
    "#  Saves Generator and Discriminator Models\n",
    "# ---------------------\n",
    "def save_GAN_models():\n",
    "    torch.save(generator.state_dict(), 'GeneratorModel.pt')\n",
    "    torch.save(discriminator.state_dict(), 'DiscriminatorModel.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:66: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.857973\n"
     ]
    }
   ],
   "source": [
    "# ---------------------\n",
    "#  Test for Classifier\n",
    "# ---------------------\n",
    "def test_classifier():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for x, y in testloader:\n",
    "        x, y = arrange_data_tensors(x, y)\n",
    "        predsProbs, preds = predict(x.flatten(1), y)\n",
    "        total += parameters[\"batch_size\"]\n",
    "        correct += (preds == y).sum().item()\n",
    "    print(\"Accuracy: %f\" % (correct / total))\n",
    "\n",
    "test_classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating mask to train model with only critic dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\pyro\\primitives.py:406: FutureWarning: The `random_module` primitive is deprecated, and will be removed in a future release. Use `pyro.nn.Module` to create Bayesian modules from `torch.nn.Module` instances.\n",
      "  \"modules from `torch.nn.Module` instances.\", FutureWarning)\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:66: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "# ---------------------\n",
    "#  Trains or loads classifier model\n",
    "# ---------------------\n",
    "def create_classifier_model():\n",
    "    if (not os.path.exists('ClassifierModel.pt') or not os.path.exists('paramstore.out')):\n",
    "        train_classifier_model()\n",
    "        test_classifier()\n",
    "    else:\n",
    "        pyro.get_param_store().load('paramstore.out')\n",
    "        model.load_state_dict(torch.load('ClassifierModel.pt'))\n",
    "\n",
    "create_classifier_model()\n",
    "mask = []\n",
    "\n",
    "\n",
    "# ---------------------\n",
    "#  Creates Mask To Train Discriminator With P_Critic\n",
    "# ---------------------\n",
    "def create_critic_mask():\n",
    "    print(\"Creating mask to train model with only critic dataset...\")\n",
    "    for i in range(int(len(trainset) / parameters[\"batch_size\"])):\n",
    "        # obtain one batch of training images\n",
    "        dataiter = iter(trainloader)\n",
    "        images, labels = dataiter.next()\n",
    "        images, labels = arrange_data_tensors(images, labels)\n",
    "\n",
    "        predsProbs, preds= predict(images, labels)\n",
    "        # convert output probabilities to predicted class\n",
    "        # predsProbs, preds = torch.max(output, 1)\n",
    "        for j in range(parameters[\"batch_size\"]):\n",
    "            mask.append(1 if predsProbs[j].item() > parameters[\"critic_threshold\"] else 0)\n",
    "\n",
    "\n",
    "create_critic_mask()\n",
    "mask = FloatTensor(mask)\n",
    "\n",
    "\n",
    "# ---------------------\n",
    "#  Sampler Class to use mask\n",
    "# ---------------------\n",
    "class SpecialSampler(torch.utils.data.sampler.Sampler):\n",
    "    def __init__(self, mask, data_source):\n",
    "        self.mask = mask\n",
    "        self.data_source = data_source\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter([i.item() for i in torch.nonzero(mask)])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_source)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------\n",
    "#  Calculates Transformation Loss While Calculating Discriminator Loss\n",
    "# ---------------------\n",
    "def calculate_transformation_loss(img, label):\n",
    "    loss = 0\n",
    "    loss += svi.evaluate_loss(img.flatten(1).to(device), label.to(device))\n",
    "    return loss/(parameters[\"batch_size\"]**2)\n",
    "\n",
    "\n",
    "# ---------------------\n",
    "#  Loads Models Or Trains Classifier, Generator, Discriminator Models\n",
    "# ---------------------\n",
    "def load_or_train_models():\n",
    "    create_classifier_model()\n",
    "\n",
    "    if parameters[\"continue_on_existing_training\"] or (\n",
    "            not (os.path.exists('DiscriminatorModel.pt') or os.path.exists('GeneratorModel.pt'))):\n",
    "        sampler = SpecialSampler(mask, trainset)\n",
    "        special_loader = torch.utils.data.DataLoader(\n",
    "            trainset,\n",
    "            drop_last=True,\n",
    "            batch_size=parameters[\"batch_size\"],\n",
    "            sampler=sampler,\n",
    "            shuffle=False\n",
    "        )\n",
    "        if parameters[\"continue_on_existing_training\"] == True and (\n",
    "                os.path.exists('DiscriminatorModel.pt') and os.path.exists('GeneratorModel.pt')):\n",
    "            load_GAN_models()\n",
    "\n",
    "        train_GAN(special_loader)\n",
    "    else:\n",
    "        load_GAN_models()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------\n",
    "#  Generator Class\n",
    "# ---------------------\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        def block(in_feat, out_feat, normalize=True):\n",
    "            layers = [nn.Linear(in_feat, out_feat)]\n",
    "            if normalize:\n",
    "                layers.append(nn.BatchNorm1d(out_feat, 0.8))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "\n",
    "        self.fc = nn.Linear(parameters[\"latent_dim\"], parameters[\"channels\"] * parameters[\"img_size\"] ** 2)\n",
    "\n",
    "        self.init_size = parameters[\"img_size\"] // 4  # Initial size before upsampling\n",
    "\n",
    "        self.l1 = nn.Sequential(nn.Conv2d(parameters[\"channels\"] * 2, 64, 3, 1, 1), nn.ReLU(inplace=True))\n",
    "\n",
    "        self.conv_blocks_for_image = nn.Sequential(\n",
    "            nn.Conv2d(1, 1, 3, stride=2, padding=1),\n",
    "            nn.Dropout2d(0.1),\n",
    "            nn.BatchNorm2d(1, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "\n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout2d(0.25),\n",
    "            nn.Conv2d(128, 256, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout2d(0.5),\n",
    "            nn.Conv2d(256, 128, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout2d(0.5),\n",
    "            nn.Conv2d(128, 64, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout2d(0.5),\n",
    "            nn.Conv2d(64, 32, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(32, 16, 3, 1, 1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout2d(0.2),\n",
    "            nn.BatchNorm2d(16, 0.8),\n",
    "            nn.Conv2d(16, 1, 3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "        self.l2 = nn.Sequential(\n",
    "            nn.Linear(4 * parameters[\"img_size\"] ** 2, parameters[\"channels\"] * parameters[\"img_size\"] ** 2),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, z, img):\n",
    "        # img = play_with_image(img)\n",
    "        first_z = self.fc(z).view(\n",
    "            [parameters[\"batch_size\"], 1, int(parameters[\"img_size\"]), int(parameters[\"img_size\"])])\n",
    "        gen_input = torch.cat((FloatTensor(img), first_z), 1)\n",
    "        out = self.l1(gen_input)\n",
    "        generated_img = self.conv_blocks(out)\n",
    "        return generated_img\n",
    "\n",
    "\n",
    "# ---------------------\n",
    "#  Discriminator Class\n",
    "# ---------------------\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(int(np.prod(img_shape)) + parameters[\"n_classes\"], 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(256, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, img, labels):\n",
    "        gen_input = torch.cat(\n",
    "            (img.view(parameters[\"img_size\"] * parameters[\"img_size\"], parameters[\"batch_size\"]),\n",
    "             labels.view(parameters[\"n_classes\"], parameters[\"batch_size\"])), 0). \\\n",
    "            view(parameters[\"batch_size\"], parameters[\"img_size\"] * parameters[\"img_size\"] + parameters[\"n_classes\"])\n",
    "        validity = self.model(gen_input)\n",
    "        return validity\n",
    "\n",
    "\n",
    "generator = Generator()\n",
    "discriminator = Discriminator()\n",
    "\n",
    "if cuda:\n",
    "    generator.cuda()\n",
    "    discriminator.cuda()\n",
    "\n",
    "generator_optimizer = torch.optim.Adam(generator.parameters(), lr=parameters[\"lr_GAN\"],\n",
    "                                       betas=(parameters[\"b1\"], parameters[\"b2\"]))\n",
    "discriminator_optimizer = torch.optim.Adam(discriminator.parameters(), lr=parameters[\"lr_GAN\"],\n",
    "                                           betas=(parameters[\"b1\"], parameters[\"b2\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------\n",
    "#  Computes Gradient Penalty\n",
    "# ---------------------\n",
    "def compute_gradient_penalty(discriminator, real_samples, fake_samples, hot_labels):\n",
    "    alpha = FloatTensor(np.random.random((real_samples.size(0), 1, 1, 1)))\n",
    "    interpolates = (alpha * real_samples + ((1 - alpha) * fake_samples)).requires_grad_(True)\n",
    "    gradients = autograd.grad(\n",
    "        discriminator(interpolates, hot_labels),\n",
    "        interpolates,\n",
    "        Variable(FloatTensor(parameters[\"batch_size\"], 1).fill_(1.0), requires_grad=False),\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "        only_inputs=True,\n",
    "    )\n",
    "    gradients = gradients[0].view(gradients[0].size(0), -1)\n",
    "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "    return gradient_penalty\n",
    "\n",
    "# ---------------------\n",
    "#  Creates One Hot Label Representation\n",
    "# ---------------------\n",
    "def create_one_hot_label(labels):\n",
    "    hot_labels = []\n",
    "    hot_label = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "    for i in range(len(labels)):\n",
    "        hot_label[labels[i]] = 1\n",
    "        hot_labels.append(hot_label)\n",
    "\n",
    "    return FloatTensor(hot_labels)\n",
    "\n",
    "\n",
    "# ---------------------\n",
    "#  Calculates Discriminator Loss\n",
    "# ---------------------\n",
    "def calculate_discriminator_loss(real_images, real_labels, fake_images, hot_labels):\n",
    "    real_validity = discriminator(real_images, hot_labels)\n",
    "    fake_validity = discriminator(fake_images, hot_labels)\n",
    "\n",
    "    # Calculating gradient penalty\n",
    "    gradient_penalty = compute_gradient_penalty(discriminator, real_images.data, fake_images.data, hot_labels)\n",
    "    # Calculating transformation penalty\n",
    "    transformation_loss = calculate_transformation_loss(fake_images, real_labels)\n",
    "\n",
    "    # Calculating total penalty\n",
    "    total_loss = -torch.mean(real_validity) + \\\n",
    "                 torch.mean(fake_validity) + \\\n",
    "                 parameters[\"lambda_gp\"] * gradient_penalty + \\\n",
    "                 parameters[\"lambda_loss\"] * transformation_loss\n",
    "\n",
    "    return total_loss\n",
    "\n",
    "\n",
    "# ---------------------\n",
    "#  Prints GAN progress\n",
    "# ---------------------\n",
    "def print_progress(epoch, d_loss, g_loss):\n",
    "    print(\"[Epoch %d/%d] [D loss: %f] [G loss: %f]\"\n",
    "          % (epoch, parameters[\"n_epochs_GAN\"], d_loss.item(), g_loss.item())\n",
    "          )\n",
    "\n",
    "    \n",
    "# ---------------------\n",
    "#  Converts Images and Labels\n",
    "# ---------------------\n",
    "def arrange_data_tensors(images, labels):\n",
    "    images = Variable(images.type(FloatTensor))\n",
    "    labels = Variable(labels.type(LongTensor))\n",
    "\n",
    "    return images, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------------\n",
    "#  Loads Generator and Discriminator Models\n",
    "# ---------------------\n",
    "def load_GAN_models():\n",
    "    generator.load_state_dict(torch.load('GeneratorModel.pt'))\n",
    "    discriminator.load_state_dict(torch.load('DiscriminatorModel.pt'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------\n",
    "#  Generates image from generator and saves if save parameter is true\n",
    "# ---------------------\n",
    "def sample_single_image(images, index, image_path, save):\n",
    "    z = Variable(FloatTensor(np.random.normal(0, 1, (parameters[\"batch_size\"], parameters[\"latent_dim\"]))))\n",
    "    generated_images = generator(z, images)\n",
    "    if (save == True):\n",
    "        save_image(generated_images[index].data, image_path, nrow=1, normalize=True)\n",
    "\n",
    "    return generated_images[index].data\n",
    "\n",
    "\n",
    "# ---------------------\n",
    "#  Shows the given image\n",
    "# ---------------------\n",
    "def show_image(img):\n",
    "    img = img.view(parameters[\"img_size\"], parameters[\"img_size\"])\n",
    "    img = img.type(torch.FloatTensor).detach().numpy()\n",
    "    plt.imshow(img, cmap='gray')\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Models And Printing Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test_models produces quantitive and qualitive results.\n",
    "\n",
    "1)Quantitive result is generating a handwritten number which has a certainty that is under threshold.(Figure 2 from Paper)\n",
    "\n",
    "2)Qualitive result is printing three rows of the \"Table 1\" from the paper.\n",
    "It produces results for [0.7, 0.8, 0.9] thresholds (Three rows of the Table 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating images and printing results...\n",
      "-------------------Results for Threshold: 0.700000 -------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:66: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Qualitative Results for Threshold----------\n",
      "Original Image : 0.518531 (Prediction Acc) || Old Label:  4\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Image: 1.000000 (Prediction Acc) || New Label:  9\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAOoUlEQVR4nO3dX6yU9Z3H8c+HPxqk1eAfWJbiFomRXY0rK0Fjmw2LsXG90V50Uy5WmpieJuqmTXrhn72ol2azbbM3klA1hbWrNmmNGJu1Bhp1bxrBsAolrYjYHiCwiBEMKsL57sV5bI545vcM88wzM/B9v5KTOWe+88x8mXM+PM/Mb37PzxEhAOe+GcNuAMBgEHYgCcIOJEHYgSQIO5DErEE+mG3e+gdaFhGe7vpGe3bbt9r+ve3dtu/vZpsZM2Z0/AJQz3bHr+J2vY6z254p6Q+SbpE0LulVSWsi4neFbaIU6omJiZ56ATIphToiWtmzr5S0OyL2RMQJSU9Jur3B/QFoUZOwL5L0pyk/j1fXfYbtMdtbbW9t8FgAGmryBt10hwqfe00QEeslrZd4gw4YpiZ79nFJi6f8/CVJ+5u1A6AtTcL+qqQrbS+xfZ6kb0ra1J+2APRbz4fxEXHS9r2SXpA0U9LjEbGzbjvecQea6XkEbZBTXHnNDrSvlQ/VADh7EHYgCcIOJEHYgSQIO5AEYQeSGOh8dql+xg6AdrBnB5Ig7EAShB1IgrADSRB2IAnCDiQx8KG3s1XdmTvbxJAk+oE9O5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kwTg7RlbdZxv4/MGZYc8OJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kwzt6lc3VMt+15+jNm9L4/qXvO6+qctvyzGoXd9l5JxySdknQyIlb0oykA/dePPfs/RMThPtwPgBbxmh1IomnYQ9KvbW+zPTbdDWyP2d5qe2vDxwLQgJu8UWH7LyNiv+35kl6U9C8R8XLh9sGbJqPlbH6DbmJioljP+rcWEdP+wxvt2SNif3V5SNIzklY2uT8A7ek57Lbn2v7ip99L+pqkHf1qDEB/NXk3foGkZ6pDpVmS/isi/rsvXY2gJoe7TQ8Zmzx23bZ1h9lz5swp1pcuXVqsr1q1qmNt9+7dxW23bdtWrB88eLBYLz3vGefK9xz2iNgj6W/72AuAFjH0BiRB2IEkCDuQBGEHkiDsQBJMce1Sm0MxTT/FVtp+1qzyr/jGG28s1rds2VKsz5w5s1hv4umnny7Wx8am/YT2nx0/frxj7dSpUz31dDZjzw4kQdiBJAg7kARhB5Ig7EAShB1IgrADSTDOPgBtnw2mNNa9bNmy4rZPPPFEsX7fffcV6xs3bizWP/zww4612bNnF7ddvXp1sV43/fajjz7qWKs7y825OMWVPTuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJME4ex+0OR+9m/oVV1zRsfboo48Wt123bl2x/tRTTxXr7777brFeGq+umwt/5MiRYv38888v1pusRnMu4tkAkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQYZx8BdePoF110UbF+9913d6w98sgjxW2ff/75Yv39998v1ttcTrpuvnvGOelN1O7ZbT9u+5DtHVOuu9j2i7bfrC7ntdsmgKa6OYz/qaRbT7vufkmbI+JKSZurnwGMsNqwR8TLkk7/3OLtkjZU32+QdEef+wLQZ72+Zl8QEQckKSIO2J7f6Ya2xySVF+UC0LrW36CLiPWS1kuSbd4xAYak16G3g7YXSlJ1eah/LQFoQ69h3yRpbfX9WknP9qcdAG2pPYy3/aSkVZIutT0u6QeSHpb0c9t3SfqjpG+02eQoaPPc7+edd16xfsMNNxTr+/bt61h77rnnitvWjaPXaTKWXTff/Prrry/W33rrrWK9bhw+m9qwR8SaDqWb+9wLgBbxcVkgCcIOJEHYgSQIO5AEYQeS8CCnAdqO0hDWKE9JbHPo7eqrry7Wr7nmmmJ9+/btHWtvv/12cdu657xu+OrUqVPFesmCBQuK9f379xfrdctR79mzp2Ot7t91Ng/bRcS0f6zs2YEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCU4l3Qd1Y/B1SxNv27atWF+9enWx/s477xTrJXXTTE+ePFms143Tl/7t1157bXHbjz/+uFh/7733ivVSb6P8mY62sGcHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQYZ++DurHqq666qlivO5V03Th+aSy86Th63bzuJqeDvvzyy4vbLl++vFivOw12xrH0EvbsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AE4+x9UDdOfvPNzRa8veyyy4r10pzxEydOFLetG4ueM2dOsX78+PFiveTOO+8s1uvOed/knPUZ1e7ZbT9u+5DtHVOue8j2Ptvbq6/b2m0TQFPdHMb/VNKt01z/44i4rvr6VX/bAtBvtWGPiJclHRlALwBa1OQNunttv14d5s/rdCPbY7a32t7a4LEANNRr2NdJWirpOkkHJP2w0w0jYn1ErIiIFT0+FoA+6CnsEXEwIk5FxISkn0ha2d+2APRbT2G3vXDKj1+XtKPTbQGMhtpxdttPSlol6VLb45J+IGmV7eskhaS9kr7TYo8jr27O90svvdTo/h944IGeH7/usVeuLB+UvfDCC8V6E+Pj48V63Vz7Opw3/rNqwx4Ra6a5+rEWegHQIj4uCyRB2IEkCDuQBGEHkiDsQBJMce2DummkO3fuLNaXLFnSqD537tyOtU8++aS47YUXXlist+mVV15ptH3G4bMm2LMDSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBIe5Fil7SgtPzzK46Z1yya3ed+zZ88u1mfN6vxxibpTPZdOQy3VT5G96aabivVbbrmlY23Lli3FbeuWg66bWpx1imtETPsHxZ4dSIKwA0kQdiAJwg4kQdiBJAg7kARhB5JgnL1LbY6zN33s0nh0k7FoqX7J5mPHjhXrixcv7lg7fPhwcdu63uuWbB7lv6c2Mc4OJEfYgSQIO5AEYQeSIOxAEoQdSIKwA0lw3vizQN14calemusu1Y9Vz58/v1ivU5qLXzdfvemSzfis2j277cW2f2N7l+2dtr9bXX+x7Rdtv1ldzmu/XQC96uYw/qSk70fEX0u6UdI9tv9G0v2SNkfElZI2Vz8DGFG1YY+IAxHxWvX9MUm7JC2SdLukDdXNNki6o60mATR3Rq/ZbX9Z0nJJv5W0ICIOSJP/Idie9sWd7TFJY83aBNBU12G3/QVJv5D0vYg42u3EkIhYL2l9dR85ZyYAI6CroTfbszUZ9J9FxC+rqw/aXljVF0o61E6LAPqhds/uyV34Y5J2RcSPppQ2SVor6eHq8tlWOjwHNJ1qWXcUVbr/uqG1uvtetmxZsf7BBx8U60ePHu1Yq5vCiv7q5jD+K5L+WdIbtrdX1z2oyZD/3PZdkv4o6RvttAigH2rDHhH/I6nTf/8397cdAG3h47JAEoQdSIKwA0kQdiAJwg4kwRTXLo3yaYmb9Fa3ZPOmTZuK9c2bNxfrpSWjm46zj/LvZBSxZweSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJBhnPwc0GW++4IILivXSqaAl6ZJLLinWS2PpTZeTxplhzw4kQdiBJAg7kARhB5Ig7EAShB1IgrADSTDOfhZoc7x50aJFjbafM2dOsV5alrluSWbG2fuLPTuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJOG6sUzbiyVtlPQXkiYkrY+I/7D9kKRvS/q/6qYPRsSvau4rSuuBM67af3Xrr9fNZ7/nnnuK9XXr1hXrpfXb+X23IyKm/aV386Gak5K+HxGv2f6ipG22X6xqP46If+9XkwDa08367AckHai+P2Z7l6RmH7sCMHBn9Jrd9pclLZf02+qqe22/bvtx2/M6bDNme6vtrY06BdBI12G3/QVJv5D0vYg4KmmdpKWSrtPknv+H020XEesjYkVErOhDvwB61FXYbc/WZNB/FhG/lKSIOBgRpyJiQtJPJK1sr00ATdWG3ZNv5z4maVdE/GjK9Qun3Ozrknb0vz0A/dLN0NtXJb0i6Q1NDr1J0oOS1mjyED4k7ZX0nerNvNJ9MdYyYkpTUKX6U0mfOHGiWGd4bfA6Db3Vhr2fCPvoIeznnk5h5xN0QBKEHUiCsANJEHYgCcIOJEHYgSQGPvRWGuqpW8IXZ65uimtTDK2NHobegOQIO5AEYQeSIOxAEoQdSIKwA0kQdiCJQS/ZfHhiYuKdKT9fKunwgHvo1qj2dkZ9DXgcfFSfMylPb3/VqTDQD9V87sHtraN6brpR7W1U+5LorVeD6o3DeCAJwg4kMeywrx/y45eMam+j2pdEb70aSG9Dfc0OYHCGvWcHMCCEHUhiKGG3favt39vebfv+YfTQie29tt+wvX3Y69NVa+gdsr1jynUX237R9pvV5bRr7A2pt4ds76ueu+22bxtSb4tt/8b2Lts7bX+3un6oz12hr4E8bwN/zW57pqQ/SLpF0rikVyWtiYjfDbSRDmzvlbQiIob+AQzbfy/pA0kbI+Ka6rp/k3QkIh6u/qOcFxH3jUhvD0n6YNjLeFerFS2cusy4pDskfUtDfO4Kff2TBvC8DWPPvlLS7ojYExEnJD0l6fYh9DHyIuJlSUdOu/p2SRuq7zdo8o9l4Dr0NhIi4kBEvFZ9f0zSp8uMD/W5K/Q1EMMI+yJJf5ry87hGa733kPRr29tsjw27mWks+HSZrepy/pD7OV3tMt6DdNoy4yPz3PWy/HlTwwj7dOfHGqXxv69ExN9J+kdJ91SHq+hOV8t4D8o0y4yPhF6XP29qGGEfl7R4ys9fkrR/CH1MKyL2V5eHJD2j0VuK+uCnK+hWl4eG3M+fjdIy3tMtM64ReO6Gufz5MML+qqQrbS+xfZ6kb0raNIQ+Psf23OqNE9meK+lrGr2lqDdJWlt9v1bSs0Ps5TNGZRnvTsuMa8jP3dCXP4+IgX9Juk2T78i/Jelfh9FDh76ukPS/1dfOYfcm6UlNHtZ9oskjorskXSJps6Q3q8uLR6i3/9Tk0t6vazJYC4fU21c1+dLwdUnbq6/bhv3cFfoayPPGx2WBJPgEHZAEYQeSIOxAEoQdSIKwA0kQdiAJwg4k8f+9JQE2astV2gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Qualitative Results for Threshold----------\n",
      "\n",
      "----------Quantitative Results for Threshold----------\n",
      "Reject            : 0.060000\n",
      "BNN Accuracy      : 40.000000\n",
      "BNN+GWIN Accuracy : 48.333333\n",
      "Rejected Accuracy :  8\n",
      "Overall Accuracy  : 0.005000\n",
      "Error             : 13.888889\n",
      "----------Quantitative Results for Threshold----------\n",
      "-------------------Results for Threshold: 0.700000 -------------------\n",
      "\n",
      "\n",
      "-------------------Results for Threshold: 0.800000 -------------------\n",
      "----------Qualitative Results for Threshold----------\n",
      "Original Image : 0.550789 (Prediction Acc) || Old Label:  5\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Image: 1.000000 (Prediction Acc) || New Label:  2\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Qualitative Results for Threshold----------\n",
      "\n",
      "----------Quantitative Results for Threshold----------\n",
      "Reject            : 0.104000\n",
      "BNN Accuracy      : 33.653846\n",
      "BNN+GWIN Accuracy : 53.846154\n",
      "Rejected Accuracy : 20\n",
      "Overall Accuracy  : 0.021000\n",
      "Error             : 19.415680\n",
      "----------Quantitative Results for Threshold----------\n",
      "-------------------Results for Threshold: 0.800000 -------------------\n",
      "\n",
      "\n",
      "-------------------Results for Threshold: 0.900000 -------------------\n",
      "----------Qualitative Results for Threshold----------\n",
      "Original Image : 0.828774 (Prediction Acc) || Old Label:  8\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Image: 1.000000 (Prediction Acc) || New Label:  9\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Qualitative Results for Threshold----------\n",
      "\n",
      "----------Quantitative Results for Threshold----------\n",
      "Reject            : 0.152000\n",
      "BNN Accuracy      : 36.184211\n",
      "BNN+GWIN Accuracy : 58.552632\n",
      "Rejected Accuracy : 22\n",
      "Overall Accuracy  : 0.034000\n",
      "Error             : 14.716066\n",
      "----------Quantitative Results for Threshold----------\n",
      "-------------------Results for Threshold: 0.900000 -------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ---------------------\n",
    "#  Test\n",
    "# ---------------------\n",
    "def test_models():\n",
    "    load_or_train_models()\n",
    "    print(\"Generating images and printing results...\")\n",
    "    for threshold_index in range(len(parameters[\"threshold\"])):\n",
    "        print('-------------------Results for Threshold: %2f -------------------' % parameters[\"threshold\"][threshold_index])\n",
    "        counter = 0\n",
    "        probUnderThreshold = 0\n",
    "        probUnderThresholdButCorrectClassified = 0\n",
    "        probUnderThresholdThenClassifiedCorrectClassified = 0\n",
    "        probAboveThresholdAndCorrectClassified = 0\n",
    "        probAboveThresholdAlsoClassifiedCorrectClassified = 0\n",
    "\n",
    "        for i in range(10):\n",
    "            for j in range(int(len(testset) / parameters[\"batch_size\"])):\n",
    "                # obtain one batch of training images\n",
    "                dataiter = iter(testloader)\n",
    "                images, labels = dataiter.next()\n",
    "\n",
    "                images, labels = arrange_data_tensors(images, labels)\n",
    "\n",
    "                # get sample outputs\n",
    "                output = model(images)\n",
    "\n",
    "                predsProbs, preds = predict(images, labels)\n",
    "\n",
    "                # convert output probabilities to predicted class\n",
    "                # predsProbs, preds = torch.max(output, 1)\n",
    "\n",
    "                for k in range(parameters[\"batch_size\"]):\n",
    "\n",
    "                    idx = (i * int(len(testset))) + (j * parameters[\"batch_size\"]) + k\n",
    "\n",
    "                    trueLabel = labels[k].item()\n",
    "                    modelsPrediction = preds[k].item()\n",
    "                    modelsPredictionProb = predsProbs[k].item()\n",
    "\n",
    "                    if (modelsPredictionProb < parameters[\"threshold\"][threshold_index]):\n",
    "                        probUnderThreshold = probUnderThreshold + 1\n",
    "                        if (trueLabel == modelsPrediction):\n",
    "                            probUnderThresholdButCorrectClassified = probUnderThresholdButCorrectClassified + 1\n",
    "                    else:\n",
    "                        if (trueLabel == modelsPrediction):\n",
    "                            probAboveThresholdAndCorrectClassified = probAboveThresholdAndCorrectClassified + 1\n",
    "\n",
    "                    if (predsProbs[k] < parameters[\"threshold\"][threshold_index]):\n",
    "                        save_image(images[k], \"images/result_images/%d\" % idx + \"_original_image.png\", nrow=1,\n",
    "                                   normalize=True)\n",
    "\n",
    "                        generatedImage = sample_single_image(images, k,\n",
    "                                                             \"images/result_images/%d\" % idx + \"_generated_image_.png\",\n",
    "                                                             True)\n",
    "                        lastPredsProbs, lastPreds = predict(generatedImage.view(1, 1, 28, 28), labels)\n",
    "\n",
    "                        if not modelsPrediction == trueLabel and lastPreds.item() == trueLabel and counter < 1:\n",
    "                            print('----------Qualitative Results for Threshold----------')\n",
    "                            print('Original Image : %2f (Prediction Acc) || Old Label: %2d' % (predsProbs[k], preds[k]))\n",
    "                            show_image(images[k])\n",
    "                            print(\"Generated Image: %2f (Prediction Acc) || New Label: %2d\" % (lastPredsProbs.item(), lastPreds.item()))\n",
    "                            show_image(generatedImage)\n",
    "                            counter = counter + 1\n",
    "                            print('----------Qualitative Results for Threshold----------\\n')\n",
    "\n",
    "                        if (lastPreds.item() == trueLabel):\n",
    "                            probUnderThresholdThenClassifiedCorrectClassified = probUnderThresholdThenClassifiedCorrectClassified + 1\n",
    "\n",
    "        total_images = 100000.0\n",
    "\n",
    "        reject = (float(100 * probUnderThreshold) / total_images)\n",
    "        bnnAcc = (100 * probUnderThresholdButCorrectClassified / probUnderThreshold)\n",
    "        bnn_and_gwinAcc = (100 * probUnderThresholdThenClassifiedCorrectClassified / probUnderThreshold)\n",
    "        overallAcc = (float(100 * (\n",
    "                probUnderThresholdThenClassifiedCorrectClassified - probUnderThresholdButCorrectClassified)) / total_images)\n",
    "        error = (100 * (bnn_and_gwinAcc - bnnAcc)) / probUnderThreshold\n",
    "\n",
    "        print('----------Quantitative Results for Threshold----------')\n",
    "        print('Reject            : %2f' % reject)\n",
    "        print('BNN Accuracy      : %2f' % bnnAcc)\n",
    "        print('BNN+GWIN Accuracy : %2f' % bnn_and_gwinAcc)\n",
    "        print('Rejected Accuracy : %2d' % (bnn_and_gwinAcc - bnnAcc))\n",
    "        print('Overall Accuracy  : %2f' % overallAcc)\n",
    "        print('Error             : %2f' % error)\n",
    "        print('----------Quantitative Results for Threshold----------')\n",
    "\n",
    "        print('-------------------Results for Threshold: %2f -------------------\\n\\n' % parameters[\"threshold\"][threshold_index])\n",
    "\n",
    "        \n",
    "test_models()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
