{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paper Information :\n",
    "    * Generative Well-intentioned Networks \n",
    "    * https://papers.nips.cc/paper/9467-generative-well-intentioned-networks.pdf\n",
    "\n",
    "Authors of code:\n",
    "    * Yasin Berk GÃ¼ltekin - 1942119\n",
    "    * Hasan Ali Duran - 1942119\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenges Encountered When Implementing Paper:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* There was not enough detail about how BNN was implemented. The number of layers was not specified. \n",
    "We could not fully obtain the BNN results mentioned in Paper. The BNN we have implemented makes predictions with higher scores. \n",
    "This situation caused difficulties in the exact occurrence of qualitative results.\n",
    "\n",
    "* The biggest problem we encountered during GAN implementation was that it was not clear enough how the\n",
    "Generator and Discriminator inputs should be processed in the model. It was not specified how many layers \n",
    "or what types of layers were used. In addition, the pictures produced by the generator appeared similar to those given\n",
    "as input to the Generator. There was no explanation for how this problem was solved in paper implementation. The new method which is transformation loss used during the Discriminator's loss calculation was not sufficiently explained.(You can find our assumptions for the models(like number of layers and type of layers) in the implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-user install because site-packages writeable\n",
      "Created temporary directory: C:\\Users\\Lenovo\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-x0ausrdi\n",
      "Created temporary directory: C:\\Users\\Lenovo\\AppData\\Local\\Temp\\pip-req-tracker-ntczw9ee\n",
      "Initialized build tracking at C:\\Users\\Lenovo\\AppData\\Local\\Temp\\pip-req-tracker-ntczw9ee\n",
      "Created build tracker: C:\\Users\\Lenovo\\AppData\\Local\\Temp\\pip-req-tracker-ntczw9ee\n",
      "Entered build tracker: C:\\Users\\Lenovo\\AppData\\Local\\Temp\\pip-req-tracker-ntczw9ee\n",
      "Created temporary directory: C:\\Users\\Lenovo\\AppData\\Local\\Temp\\pip-install-vmsntved\n",
      "Requirement already satisfied: pyro-ppl in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (1.3.1)\n",
      "Requirement already satisfied: pyro-api>=0.1.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from pyro-ppl) (0.1.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from pyro-ppl) (3.2.1)\n",
      "Requirement already satisfied: tqdm>=4.36 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from pyro-ppl) (4.42.1)\n",
      "Requirement already satisfied: torch>=1.4.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from pyro-ppl) (1.5.0)\n",
      "Requirement already satisfied: numpy>=1.7 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from pyro-ppl) (1.18.1)\n",
      "Requirement already satisfied: future in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from torch>=1.4.0->pyro-ppl) (0.18.2)\n",
      "Cleaning up...\n",
      "Removed build tracker: 'C:\\\\Users\\\\Lenovo\\\\AppData\\\\Local\\\\Temp\\\\pip-req-tracker-ntczw9ee'\n"
     ]
    }
   ],
   "source": [
    "!pip install -v pyro-ppl\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets\n",
    "import os.path\n",
    "import os\n",
    "import numpy as np\n",
    "from torchvision.utils import save_image\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.autograd as autograd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import subprocess\n",
    "import pyro.distributions as dist\n",
    "import pyro\n",
    "from matplotlib import colors\n",
    "from pyro import optim\n",
    "from pyro.infer import SVI, Trace_ELBO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HyperParameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    \"threshold\": [0.7, 0.8, 0.9],\n",
    "    \"critic_threshold\": 0.95,\n",
    "    \"n_critic\": 5,\n",
    "    \"n_epochs_Classifier\": 30,\n",
    "    \"n_epochs_GAN\": 200000,\n",
    "    \"batch_size\": 128,\n",
    "    \"lr_Classifier\": 0.001, # learning rate for Classifier model\n",
    "    \"lr_GAN\": 0.0001,  # learning rate for GAN models\n",
    "    \"b1\": 0.5,\n",
    "    \"b2\": 0.9,\n",
    "    \"latent_dim\": 100,\n",
    "    \"n_classes\": 10,\n",
    "    \"img_size\": 28,\n",
    "    \"channels\": 1,\n",
    "    \"lambda_gp\": 10, # lambda for gradient penalty\n",
    "    \"lambda_loss\": 10, # lambda for transformation penalty\n",
    "    \"continue_on_existing_training\": 0,\n",
    "    \"cuda\": 1,\n",
    "    \"run_download_sh\": 0\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_shape = (parameters[\"channels\"], parameters[\"img_size\"], parameters[\"img_size\"])\n",
    "\n",
    "cuda = True if parameters[\"cuda\"] else False\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "FloatTensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if cuda else torch.LongTensor\n",
    "\n",
    "# ---------------------\n",
    "#  Setting Datasets\n",
    "# ---------------------\n",
    "if parameters[\"run_download_sh\"]:\n",
    "    subprocess.call(\"download.sh\", shell=True)\n",
    "\n",
    "trainset = datasets.MNIST(\n",
    "    root=\"\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.Compose(\n",
    "        [transforms.Resize(parameters[\"img_size\"]), transforms.ToTensor(), transforms.Normalize([0.5], [0.5])]))\n",
    "\n",
    "testset = datasets.MNIST(\n",
    "    root=\"\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transforms.Compose(\n",
    "        [transforms.Resize(parameters[\"img_size\"]), transforms.ToTensor(), transforms.Normalize([0.5], [0.5])]))\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset,\n",
    "    drop_last=True,\n",
    "    batch_size=parameters[\"batch_size\"],\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset,\n",
    "    drop_last=True,\n",
    "    batch_size=parameters[\"batch_size\"],\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classifier Model (Bayesian Neural Network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------\n",
    "#  BNN - Classifier Model\n",
    "# ---------------------\n",
    "class BNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BNN, self).__init__()\n",
    "\n",
    "        self.out = nn.Sequential(\n",
    "            nn.Linear(parameters[\"img_size\"] ** 2, 1024),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(1024, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.flatten(1)\n",
    "        return self.out(x)\n",
    "\n",
    "\n",
    "model = BNN()\n",
    "if cuda:\n",
    "    model.cuda()\n",
    "\n",
    "\n",
    "# ---------------------\n",
    "#  Stochastic Variational Inference's Module\n",
    "# ---------------------\n",
    "def module(x, y):\n",
    "    priors = {}\n",
    "    for iterator in model.named_parameters():\n",
    "        name, param = iterator\n",
    "        zeros = torch.zeros_like(param.data)\n",
    "        ones = torch.ones_like(param.data)\n",
    "        priors[name] = dist.Normal(loc=zeros,\n",
    "                                   scale=ones)\n",
    "\n",
    "    lifted_module = pyro.random_module(\"module\", model, priors)\n",
    "    lifted_module_method = lifted_module()\n",
    "    lhat = F.log_softmax(lifted_module_method(x), 1)\n",
    "    pyro.sample(\"obs\", dist.Categorical(logits=lhat), obs=y)\n",
    "\n",
    "\n",
    "# ---------------------\n",
    "#  Stochastic Variational Inference's Guide\n",
    "# ---------------------\n",
    "def guide(x, y):\n",
    "    priors = {}\n",
    "    for iterator in model.named_parameters():\n",
    "        name, param = iterator\n",
    "        priors[name] = dist.Normal(loc=pyro.param(name + '.mu', torch.randn_like(param)),\n",
    "                                   scale=F.softplus(pyro.param(name + '.sigma', torch.randn_like(param))))\n",
    "\n",
    "    lifted_module = pyro.random_module('module', model, priors)\n",
    "    return lifted_module()\n",
    "\n",
    "\n",
    "opt = optim.Adam({'lr': parameters[\"lr_Classifier\"]})\n",
    "svi = SVI(module, guide, opt, loss=Trace_ELBO())\n",
    "\n",
    "# ---------------------\n",
    "#  Classifier's prediction method\n",
    "# ---------------------\n",
    "def predict(x, y):\n",
    "    sampled_models = [guide(None, None) for _ in range(parameters[\"n_classes\"])]\n",
    "    yhats = [model(x.to(device)).data for model in sampled_models]\n",
    "    mean = torch.mean(torch.stack(yhats), 0)\n",
    "    predsProbs, preds = torch.max(F.softmax(mean).to(device), 1)\n",
    "    return predsProbs, preds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training And Saving Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Loss: 6414.542196\n",
      "Epoch: 2 Loss: 5750.218853\n",
      "Epoch: 3 Loss: 5376.938865\n"
     ]
    }
   ],
   "source": [
    "# ---------------------\n",
    "#  Trains Classifier Model\n",
    "# ---------------------\n",
    "def train_classifier_model():\n",
    "    pyro.clear_param_store()\n",
    "\n",
    "    total_loss = 0\n",
    "    for epoch in range(parameters[\"n_epochs_Classifier\"]):\n",
    "        loss = 0\n",
    "        for x, y in trainloader:\n",
    "            loss += svi.step(x.flatten(1).to(device), y.to(device))\n",
    "        total_loss = loss / len(trainloader.dataset)\n",
    "        print(\"Epoch: %d Loss: %f\" % (epoch + 1, total_loss))\n",
    "\n",
    "    pyro.get_param_store().save('paramstore.out')\n",
    "    torch.save(model.state_dict(), 'ClassifierModel.pt')\n",
    "\n",
    "train_classifier_model()\n",
    "\n",
    "# ---------------------\n",
    "#  Trains Generative Adverserial Network model\n",
    "# ---------------------\n",
    "def train_GAN(dataloader):\n",
    "    batches_done = 0\n",
    "    for epoch in range(parameters[\"n_epochs_GAN\"]):\n",
    "        for i, (imgs, labels) in enumerate(dataloader):\n",
    "            hot_labels = create_one_hot_label(labels)\n",
    "\n",
    "            real_images = Variable(imgs.type(FloatTensor))\n",
    "            labels = Variable(labels.type(LongTensor))\n",
    "\n",
    "            discriminator_optimizer.zero_grad()\n",
    "\n",
    "            z = Variable(FloatTensor(np.random.normal(0, 1, (parameters[\"batch_size\"], parameters[\"latent_dim\"]))))\n",
    "\n",
    "            generated_images = generator(z, real_images)\n",
    "\n",
    "            d_loss = calculate_discriminator_loss(real_images, labels, generated_images, hot_labels)\n",
    "\n",
    "            d_loss.backward()\n",
    "            discriminator_optimizer.step()\n",
    "\n",
    "            generator_optimizer.zero_grad()\n",
    "\n",
    "\n",
    "            if i % parameters[\"n_critic\"] == 0:\n",
    "\n",
    "                generated_images = generator(z, real_images)\n",
    "\n",
    "                fake_validity = discriminator(generated_images, hot_labels)\n",
    "                g_loss = -torch.mean(fake_validity)\n",
    "\n",
    "                g_loss.backward()\n",
    "                generator_optimizer.step()\n",
    "\n",
    "                print_progress(epoch, d_loss, g_loss)\n",
    "\n",
    "                batches_done += batches_done + parameters[\"n_critic\"]\n",
    "\n",
    "                if batches_done % 100 == 0:\n",
    "                    save_image(generated_images.data[:25], \"images/wgan/%d.png\" % batches_done, nrow=5, normalize=True)\n",
    "\n",
    "    save_GAN_models()\n",
    "    \n",
    "# ---------------------\n",
    "#  Saves Generator and Discriminator Models\n",
    "# ---------------------\n",
    "def save_GAN_models():\n",
    "    torch.save(generator.state_dict(), 'GeneratorModel.pt')\n",
    "    torch.save(discriminator.state_dict(), 'DiscriminatorModel.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:66: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.857973\n"
     ]
    }
   ],
   "source": [
    "# ---------------------\n",
    "#  Test for Classifier\n",
    "# ---------------------\n",
    "def test_classifier():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for x, y in testloader:\n",
    "        x, y = arrange_data_tensors(x, y)\n",
    "        predsProbs, preds = predict(x.flatten(1), y)\n",
    "        total += parameters[\"batch_size\"]\n",
    "        correct += (preds == y).sum().item()\n",
    "    print(\"Accuracy: %f\" % (correct / total))\n",
    "\n",
    "test_classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating mask to train model with only critic dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\pyro\\primitives.py:406: FutureWarning: The `random_module` primitive is deprecated, and will be removed in a future release. Use `pyro.nn.Module` to create Bayesian modules from `torch.nn.Module` instances.\n",
      "  \"modules from `torch.nn.Module` instances.\", FutureWarning)\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:66: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "# ---------------------\n",
    "#  Trains or loads classifier model\n",
    "# ---------------------\n",
    "def create_classifier_model():\n",
    "    if (not os.path.exists('ClassifierModel.pt') or not os.path.exists('paramstore.out')):\n",
    "        train_classifier_model()\n",
    "        test_classifier()\n",
    "    else:\n",
    "        pyro.get_param_store().load('paramstore.out')\n",
    "        model.load_state_dict(torch.load('ClassifierModel.pt'))\n",
    "\n",
    "create_classifier_model()\n",
    "mask = []\n",
    "\n",
    "\n",
    "# ---------------------\n",
    "#  Creates Mask To Train Discriminator With P_Critic\n",
    "# ---------------------\n",
    "def create_critic_mask():\n",
    "    print(\"Creating mask to train model with only critic dataset...\")\n",
    "    for i in range(int(len(trainset) / parameters[\"batch_size\"])):\n",
    "        # obtain one batch of training images\n",
    "        dataiter = iter(trainloader)\n",
    "        images, labels = dataiter.next()\n",
    "        images, labels = arrange_data_tensors(images, labels)\n",
    "\n",
    "        predsProbs, preds= predict(images, labels)\n",
    "        # convert output probabilities to predicted class\n",
    "        # predsProbs, preds = torch.max(output, 1)\n",
    "        for j in range(parameters[\"batch_size\"]):\n",
    "            mask.append(1 if predsProbs[j].item() > parameters[\"critic_threshold\"] else 0)\n",
    "\n",
    "\n",
    "create_critic_mask()\n",
    "mask = FloatTensor(mask)\n",
    "\n",
    "\n",
    "# ---------------------\n",
    "#  Sampler Class to use mask\n",
    "# ---------------------\n",
    "class SpecialSampler(torch.utils.data.sampler.Sampler):\n",
    "    def __init__(self, mask, data_source):\n",
    "        self.mask = mask\n",
    "        self.data_source = data_source\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter([i.item() for i in torch.nonzero(mask)])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_source)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------\n",
    "#  Calculates Transformation Loss While Calculating Discriminator Loss\n",
    "# ---------------------\n",
    "def calculate_transformation_loss(img, label):\n",
    "    loss = 0\n",
    "    loss += svi.evaluate_loss(img.flatten(1).to(device), label.to(device))\n",
    "    return loss/(parameters[\"batch_size\"]**2)\n",
    "\n",
    "\n",
    "# ---------------------\n",
    "#  Loads Models Or Trains Classifier, Generator, Discriminator Models\n",
    "# ---------------------\n",
    "def load_or_train_models():\n",
    "    create_classifier_model()\n",
    "\n",
    "    if parameters[\"continue_on_existing_training\"] or (\n",
    "            not (os.path.exists('DiscriminatorModel.pt') or os.path.exists('GeneratorModel.pt'))):\n",
    "        sampler = SpecialSampler(mask, trainset)\n",
    "        special_loader = torch.utils.data.DataLoader(\n",
    "            trainset,\n",
    "            drop_last=True,\n",
    "            batch_size=parameters[\"batch_size\"],\n",
    "            sampler=sampler,\n",
    "            shuffle=False\n",
    "        )\n",
    "        if parameters[\"continue_on_existing_training\"] == True and (\n",
    "                os.path.exists('DiscriminatorModel.pt') and os.path.exists('GeneratorModel.pt')):\n",
    "            load_GAN_models()\n",
    "\n",
    "        train_GAN(special_loader)\n",
    "    else:\n",
    "        load_GAN_models()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------\n",
    "#  Generator Class\n",
    "# ---------------------\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        def block(in_feat, out_feat, normalize=True):\n",
    "            layers = [nn.Linear(in_feat, out_feat)]\n",
    "            if normalize:\n",
    "                layers.append(nn.BatchNorm1d(out_feat, 0.8))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "\n",
    "        self.fc = nn.Linear(parameters[\"latent_dim\"], parameters[\"channels\"] * parameters[\"img_size\"] ** 2)\n",
    "\n",
    "        self.init_size = parameters[\"img_size\"] // 4  # Initial size before upsampling\n",
    "\n",
    "        self.l1 = nn.Sequential(nn.Conv2d(parameters[\"channels\"] * 2, 64, 3, 1, 1), nn.ReLU(inplace=True))\n",
    "\n",
    "        self.conv_blocks_for_image = nn.Sequential(\n",
    "            nn.Conv2d(1, 1, 3, stride=2, padding=1),\n",
    "            nn.Dropout2d(0.1),\n",
    "            nn.BatchNorm2d(1, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "\n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout2d(0.25),\n",
    "            nn.Conv2d(128, 256, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout2d(0.5),\n",
    "            nn.Conv2d(256, 128, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout2d(0.5),\n",
    "            nn.Conv2d(128, 64, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout2d(0.5),\n",
    "            nn.Conv2d(64, 32, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(32, 16, 3, 1, 1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout2d(0.2),\n",
    "            nn.BatchNorm2d(16, 0.8),\n",
    "            nn.Conv2d(16, 1, 3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "        self.l2 = nn.Sequential(\n",
    "            nn.Linear(4 * parameters[\"img_size\"] ** 2, parameters[\"channels\"] * parameters[\"img_size\"] ** 2),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, z, img):\n",
    "        # img = play_with_image(img)\n",
    "        first_z = self.fc(z).view(\n",
    "            [parameters[\"batch_size\"], 1, int(parameters[\"img_size\"]), int(parameters[\"img_size\"])])\n",
    "        gen_input = torch.cat((FloatTensor(img), first_z), 1)\n",
    "        out = self.l1(gen_input)\n",
    "        generated_img = self.conv_blocks(out)\n",
    "        return generated_img\n",
    "\n",
    "\n",
    "# ---------------------\n",
    "#  Discriminator Class\n",
    "# ---------------------\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(int(np.prod(img_shape)) + parameters[\"n_classes\"], 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(256, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, img, labels):\n",
    "        gen_input = torch.cat(\n",
    "            (img.view(parameters[\"img_size\"] * parameters[\"img_size\"], parameters[\"batch_size\"]),\n",
    "             labels.view(parameters[\"n_classes\"], parameters[\"batch_size\"])), 0). \\\n",
    "            view(parameters[\"batch_size\"], parameters[\"img_size\"] * parameters[\"img_size\"] + parameters[\"n_classes\"])\n",
    "        validity = self.model(gen_input)\n",
    "        return validity\n",
    "\n",
    "\n",
    "generator = Generator()\n",
    "discriminator = Discriminator()\n",
    "\n",
    "if cuda:\n",
    "    generator.cuda()\n",
    "    discriminator.cuda()\n",
    "\n",
    "generator_optimizer = torch.optim.Adam(generator.parameters(), lr=parameters[\"lr_GAN\"],\n",
    "                                       betas=(parameters[\"b1\"], parameters[\"b2\"]))\n",
    "discriminator_optimizer = torch.optim.Adam(discriminator.parameters(), lr=parameters[\"lr_GAN\"],\n",
    "                                           betas=(parameters[\"b1\"], parameters[\"b2\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------\n",
    "#  Computes Gradient Penalty\n",
    "# ---------------------\n",
    "def compute_gradient_penalty(discriminator, real_samples, fake_samples, hot_labels):\n",
    "    alpha = FloatTensor(np.random.random((real_samples.size(0), 1, 1, 1)))\n",
    "    interpolates = (alpha * real_samples + ((1 - alpha) * fake_samples)).requires_grad_(True)\n",
    "    gradients = autograd.grad(\n",
    "        discriminator(interpolates, hot_labels),\n",
    "        interpolates,\n",
    "        Variable(FloatTensor(parameters[\"batch_size\"], 1).fill_(1.0), requires_grad=False),\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "        only_inputs=True,\n",
    "    )\n",
    "    gradients = gradients[0].view(gradients[0].size(0), -1)\n",
    "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "    return gradient_penalty\n",
    "\n",
    "# ---------------------\n",
    "#  Creates One Hot Label Representation\n",
    "# ---------------------\n",
    "def create_one_hot_label(labels):\n",
    "    hot_labels = []\n",
    "    hot_label = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "    for i in range(len(labels)):\n",
    "        hot_label[labels[i]] = 1\n",
    "        hot_labels.append(hot_label)\n",
    "\n",
    "    return FloatTensor(hot_labels)\n",
    "\n",
    "\n",
    "# ---------------------\n",
    "#  Calculates Discriminator Loss\n",
    "# ---------------------\n",
    "def calculate_discriminator_loss(real_images, real_labels, fake_images, hot_labels):\n",
    "    real_validity = discriminator(real_images, hot_labels)\n",
    "    fake_validity = discriminator(fake_images, hot_labels)\n",
    "\n",
    "    # Calculating gradient penalty\n",
    "    gradient_penalty = compute_gradient_penalty(discriminator, real_images.data, fake_images.data, hot_labels)\n",
    "    # Calculating transformation penalty\n",
    "    transformation_loss = calculate_transformation_loss(fake_images, real_labels)\n",
    "\n",
    "    # Calculating total penalty\n",
    "    total_loss = -torch.mean(real_validity) + \\\n",
    "                 torch.mean(fake_validity) + \\\n",
    "                 parameters[\"lambda_gp\"] * gradient_penalty + \\\n",
    "                 parameters[\"lambda_loss\"] * transformation_loss\n",
    "\n",
    "    return total_loss\n",
    "\n",
    "\n",
    "# ---------------------\n",
    "#  Prints GAN progress\n",
    "# ---------------------\n",
    "def print_progress(epoch, d_loss, g_loss):\n",
    "    print(\"[Epoch %d/%d] [D loss: %f] [G loss: %f]\"\n",
    "          % (epoch, parameters[\"n_epochs_GAN\"], d_loss.item(), g_loss.item())\n",
    "          )\n",
    "\n",
    "    \n",
    "# ---------------------\n",
    "#  Converts Images and Labels\n",
    "# ---------------------\n",
    "def arrange_data_tensors(images, labels):\n",
    "    images = Variable(images.type(FloatTensor))\n",
    "    labels = Variable(labels.type(LongTensor))\n",
    "\n",
    "    return images, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------------\n",
    "#  Loads Generator and Discriminator Models\n",
    "# ---------------------\n",
    "def load_GAN_models():\n",
    "    generator.load_state_dict(torch.load('GeneratorModel.pt'))\n",
    "    discriminator.load_state_dict(torch.load('DiscriminatorModel.pt'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------\n",
    "#  Generates image from generator and saves if save parameter is true\n",
    "# ---------------------\n",
    "def sample_single_image(images, index, image_path, save):\n",
    "    z = Variable(FloatTensor(np.random.normal(0, 1, (parameters[\"batch_size\"], parameters[\"latent_dim\"]))))\n",
    "    generated_images = generator(z, images)\n",
    "    if (save == True):\n",
    "        save_image(generated_images[index].data, image_path, nrow=1, normalize=True)\n",
    "\n",
    "    return generated_images[index].data\n",
    "\n",
    "\n",
    "# ---------------------\n",
    "#  Shows the given image\n",
    "# ---------------------\n",
    "def show_image(img):\n",
    "    img = img.view(parameters[\"img_size\"], parameters[\"img_size\"])\n",
    "    img = img.type(torch.FloatTensor).detach().numpy()\n",
    "    plt.imshow(img, cmap='gray')\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Models And Printing Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test_models produces quantitive and qualitive results.\n",
    "\n",
    "1)Quantitive result is generating a handwritten number which has a certainty that is under threshold.(Figure 2 from Paper)\n",
    "\n",
    "2)Qualitive result is printing three rows of the \"Table 1\" from the paper.\n",
    "It produces results for [0.7, 0.8, 0.9] thresholds (Three rows of the Table 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating images and printing results...\n",
      "-------------------Results for Threshold: 0.700000 -------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:66: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Qualitative Results for Threshold----------\n",
      "Original Image : 0.508209 (Prediction Acc) || Old Label:  3\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Image: 1.000000 (Prediction Acc) || New Label:  5\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Qualitative Results for Threshold----------\n",
      "\n",
      "----------Quantitative Results for Threshold----------\n",
      "Reject            : 0.072000\n",
      "BNN Accuracy      : 43.055556\n",
      "BNN+GWIN Accuracy : 56.944444\n",
      "Rejected Accuracy : 13\n",
      "Overall Accuracy  : 0.010000\n",
      "Error             : 19.290123\n",
      "----------Quantitative Results for Threshold----------\n",
      "-------------------Results for Threshold: 0.700000 -------------------\n",
      "\n",
      "\n",
      "-------------------Results for Threshold: 0.800000 -------------------\n",
      "----------Qualitative Results for Threshold----------\n",
      "Original Image : 0.768581 (Prediction Acc) || Old Label:  8\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Image: 1.000000 (Prediction Acc) || New Label:  0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAOsklEQVR4nO3db4hddX7H8c/XSWIwCZIo2uCmdXf1QaOgW4IUDFVZdrE+iXmgbECxmHQWXWGFQhvsgxXKirTdloKwOOtqsmWbJWLSyBLMaljW9slilNTETbOxku7OZkhqB5KJ+T/59sGclEmc8/vd3HPOPWfm+37BcGfub+653zkznznn3u8552fuLgBz3zVtFwBgMAg7EARhB4Ig7EAQhB0IYt4gn8zMeOsfaJi720z3V9qym9mDZnbQzD42s41VllWVmSU/gOis3z67mQ1J+rWkr0kalfSepHXu/qvEYxrbsucCzfEEiKKJLfs9kj5290/c/Zykn0haU2F5ABpUJey3SPrttK9Hi/suY2bDZrbHzPZUeC4AFVV5g26mXYXP7Su7+4ikEYk36IA2Vdmyj0paMe3rL0g6Uq0cAE2pEvb3JN1uZl80swWSviHpzXrKAlC3vnfj3f2CmT0jaZekIUmvuvtHtVV29fW09dTArNB3662vJ+M1O9C4Rg6qATB7EHYgCMIOBEHYgSAIOxAEYQeCIOxAEIQdCIKwA0EQdiAIwg4EQdiBIAg7EMRALyWNwZvNF+LM1V7lZ+vyz90UtuxAEIQdCIKwA0EQdiAIwg4EQdiBIAg7EAR99h6lerpN92xz/eRrrin/n50ak6QLFy5Ueu5589J/QgsWLCgdO3v2bPKxk5OTyfGhoaHkeJVlX7x4se9ldxVbdiAIwg4EQdiBIAg7EARhB4Ig7EAQhB0Igj57j5rsped62fPnz0+Op3rGuX5yzsqVK5Pj+/bt63vZjz76aHJ827ZtyfHcz5Zbr9FUCruZHZY0IWlS0gV3X1VHUQDqV8eW/QF3/7SG5QBoEK/ZgSCqht0l/czM3jez4Zm+wcyGzWyPme2p+FwAKqi6G3+vux8xs5skvW1m/+nu707/BncfkTQiSWYW7yp/QEdU2rK7+5Hi9pik7ZLuqaMoAPXrO+xmtsjMllz6XNLXJe2vqzAA9bJ++8dm9iVNbc2lqZcD/+Lu3808prO78VV6srl1mDunfMmSJcnx3LnV586dKx07efJk8rG589HbtHXr1uT4+vXrk+NnzpwpHcudC59ap1K3rzvv7jP+Mff9m3b3TyTd1XdFAAaK1hsQBGEHgiDsQBCEHQiCsANB9N166+vJOtx6a1KuvZUbT7WQJOmOO+4oHdu/f+4e+jA6Opocv+2220rHcq3W8+fPJ8ernjrcpLLWG1t2IAjCDgRB2IEgCDsQBGEHgiDsQBCEHQiCPvsALFy4MDletafb5O8w99y5YwRSp/c23au+/vrrS8dOnz5dadm531mb6LMDwRF2IAjCDgRB2IEgCDsQBGEHgiDsQBDdvY7wLFJ1yuXcZYsfeOCBq66pLtddd12lx6cug71s2bLkY8fHxys999q1a0vHtmzZUmnZud95Fy81zZYdCIKwA0EQdiAIwg4EQdiBIAg7EARhB4LgfPYa5KZkXrRoUXI8N61ybsrmJi1YsCA5nus3p44hyD32ySefTI6/8soryfGUxYsXJ8dzxz5cuHAhOd5mn73v89nN7FUzO2Zm+6fdt8zM3jazQ8Xt0jqLBVC/XnbjN0l68Ir7Nkra7e63S9pdfA2gw7Jhd/d3JV153OIaSZuLzzdLerjmugDUrN9j42929zFJcvcxM7up7BvNbFjScJ/PA6AmjZ8I4+4jkkakufsGHTAb9Nt6O2pmyyWpuD1WX0kAmtBv2N+U9ETx+ROSdtRTDoCmZPvsZrZF0v2SbpR0VNJ3JP2rpK2Sfl/SbyQ94u7Zk4/n6m780NBQcjzXq85dw7zJnu2aNWuS4zt37kyO5372s2fPXnVNlyxdmu7oVjnf/YYbbkiOT0xMJMdnY589+5rd3deVDH21UkUABorDZYEgCDsQBGEHgiDsQBCEHQiCU1xrkJuSOefMmTPJ8SZ/R7kpl6teMrnKtMxVL8Gdkmu9HT9+PDmeO+24i603tuxAEIQdCIKwA0EQdiAIwg4EQdiBIAg7EARTNvco1W/OTWt84sSJ5HjuUtRNqtoPbvMy11Xkjh+Yi9iyA0EQdiAIwg4EQdiBIAg7EARhB4Ig7EAQ9Nl7lOrLnjp1KvnY3DndTz/9dF81DUKb52U32QvPnSs/F7FlB4Ig7EAQhB0IgrADQRB2IAjCDgRB2IEg6LPXoOr0vS+99FKd5Vxmw4YNlR7fZp+9Sblr/ed+7tm4XrJbdjN71cyOmdn+afc9b2a/M7O9xcdDzZYJoKpeduM3SXpwhvv/0d3vLj521lsWgLplw+7u70oaH0AtABpU5Q26Z8zsw2I3f2nZN5nZsJntMbM9FZ4LQEX9hv37kr4s6W5JY5K+V/aN7j7i7qvcfVWfzwWgBn2F3d2Puvuku1+U9ANJ99RbFoC69RV2M1s+7cu1kvaXfS+Absj22c1si6T7Jd1oZqOSviPpfjO7W5JLOizpmw3W2Hld7rlu2rSp0uPb/NmGhoYaW3aXf2dNyYbd3dfNcPcPG6gFQIM4XBYIgrADQRB2IAjCDgRB2IEgOMW1R1WmVW56SuaJiYnSsS63mHKXil6xYkVjz3369Onk+Fyc0pktOxAEYQeCIOxAEIQdCIKwA0EQdiAIwg4EQZ+9R6m+a+5UzHPnztVdzmUeeeSR0rFcj7/LUzJv37690vJ37NhROvbZZ58lH9vl4xP6xZYdCIKwA0EQdiAIwg4EQdiBIAg7EARhB4Kgz17I9Xyr9NmbPjc61Uuv2i+eNy/9J5Kbrjpl8eLFyfGVK1f2vWxJeuGFF0rHqk6zPRuxZQeCIOxAEIQdCIKwA0EQdiAIwg4EQdiBIOiz9yjVd82dM37ttdfWXc5l7rzzztKxt956K/nYXD+56jECqT798ePHKy0759ChQ6Vjk5OTyceG7LOb2Qoz+7mZHTCzj8zs28X9y8zsbTM7VNwubb5cAP3qZTf+gqS/cPc/lPTHkr5lZislbZS0291vl7S7+BpAR2XD7u5j7v5B8fmEpAOSbpG0RtLm4ts2S3q4qSIBVHdVr9nN7FZJX5H0S0k3u/uYNPUPwcxuKnnMsKThamUCqKrnsJvZYklvSHrW3U/0+saNu49IGimWMffe9QBmiZ5ab2Y2X1NB/7G7byvuPmpmy4vx5ZKONVMigDpYD60X09Rr8nF3f3ba/X8n6X/d/UUz2yhpmbv/ZWZZnd2y5/ZUUqexLly4MPnY+fPnJ8fHx8eT41Xk2n65y1zn1kvuZz916lRyvIqnnnoqOf7aa6+VjuVOcc215rrM3Wf8pfWyG3+vpMcl7TOzvcV9z0l6UdJWM1sv6TeSyi9eDqB12bC7+79LKvv3/tV6ywHQFA6XBYIg7EAQhB0IgrADQRB2IIhsn73WJ+twnz0n1W/OXW45dwrsfffdlxzftWtXcnyuevzxx5Pjr7/+enL8/PnzpWO5v/vZfIprWZ+dLTsQBGEHgiDsQBCEHQiCsANBEHYgCMIOBEGfvUepPnuuj151Sud33nknOb569erkeFfdddddyfGDBw8mx3PnpF+8eLF0bDb30XPoswPBEXYgCMIOBEHYgSAIOxAEYQeCIOxAEPTZa5Drs+f66FV/Bxs2bCgde/nllystO+exxx5Ljm/fvr10LHfN+tx6SfXRe3n8XEWfHQiOsANBEHYgCMIOBEHYgSAIOxAEYQeC6GV+9hWSfiTp9yRdlDTi7v9kZs9L+nNJ/1N863PuvjOzrJiNT2CAyvrsvYR9uaTl7v6BmS2R9L6khyU9Kumku/99r0UQdqB5ZWHvZX72MUljxecTZnZA0i31lgegaVf1mt3MbpX0FUm/LO56xsw+NLNXzWxpyWOGzWyPme2pVCmASno+Nt7MFkv6haTvuvs2M7tZ0qeSXNLfaGpX/8nMMtiNBxrW92t2STKz+ZJ+KmmXu//DDOO3Svqpu9+ZWQ5hBxrW94kwNnXK1g8lHZge9OKNu0vWStpftUgAzenl3fjVkv5N0j5Ntd4k6TlJ6yTdrand+MOSvlm8mZdaFlt2oGGVduPrQtiB5nE+OxAcYQeCIOxAEIQdCIKwA0EQdiAIwg4EQdiBIAg7EARhB4Ig7EAQhB0IgrADQRB2IIjsBSdr9qmk/5729Y3FfV3U1dq6WpdEbf2qs7Y/KBsY6Pnsn3tysz3uvqq1AhK6WltX65KorV+Dqo3deCAIwg4E0XbYR1p+/pSu1tbVuiRq69dAamv1NTuAwWl7yw5gQAg7EEQrYTezB83soJl9bGYb26ihjJkdNrN9Zra37fnpijn0jpnZ/mn3LTOzt83sUHE74xx7LdX2vJn9rlh3e83soZZqW2FmPzezA2b2kZl9u7i/1XWXqGsg623gr9nNbEjSryV9TdKopPckrXP3Xw20kBJmdljSKndv/QAMM/sTSScl/ejS1Fpm9reSxt39xeIf5VJ3/6uO1Pa8rnIa74ZqK5tm/M/U4rqrc/rzfrSxZb9H0sfu/om7n5P0E0lrWqij89z9XUnjV9y9RtLm4vPNmvpjGbiS2jrB3cfc/YPi8wlJl6YZb3XdJeoaiDbCfouk3077elTdmu/dJf3MzN43s+G2i5nBzZem2Spub2q5nitlp/EepCumGe/Muutn+vOq2gj7TFPTdKn/d6+7/5GkP5X0rWJ3Fb35vqQva2oOwDFJ32uzmGKa8TckPevuJ9qsZboZ6hrIemsj7KOSVkz7+guSjrRQx4zc/Uhxe0zSdk297OiSo5dm0C1uj7Vcz/9z96PuPunuFyX9QC2uu2Ka8Tck/djdtxV3t77uZqprUOutjbC/J+l2M/uimS2Q9A1Jb7ZQx+eY2aLijROZ2SJJX1f3pqJ+U9ITxedPSNrRYi2X6co03mXTjKvlddf69OfuPvAPSQ9p6h35/5L0123UUFLXlyT9R/HxUdu1Sdqiqd2685raI1ov6QZJuyUdKm6Xdai2f9bU1N4faipYy1uqbbWmXhp+KGlv8fFQ2+suUddA1huHywJBcAQdEARhB4Ig7EAQhB0IgrADQRB2IAjCDgTxfyNFBgIT4HV7AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Qualitative Results for Threshold----------\n",
      "\n",
      "----------Quantitative Results for Threshold----------\n",
      "Reject            : 0.115000\n",
      "BNN Accuracy      : 36.521739\n",
      "BNN+GWIN Accuracy : 60.000000\n",
      "Rejected Accuracy : 23\n",
      "Overall Accuracy  : 0.027000\n",
      "Error             : 20.415879\n",
      "----------Quantitative Results for Threshold----------\n",
      "-------------------Results for Threshold: 0.800000 -------------------\n",
      "\n",
      "\n",
      "-------------------Results for Threshold: 0.900000 -------------------\n",
      "----------Qualitative Results for Threshold----------\n",
      "Original Image : 0.501099 (Prediction Acc) || Old Label:  3\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Image: 1.000000 (Prediction Acc) || New Label:  2\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Qualitative Results for Threshold----------\n",
      "\n",
      "----------Quantitative Results for Threshold----------\n",
      "Reject            : 0.156000\n",
      "BNN Accuracy      : 39.743590\n",
      "BNN+GWIN Accuracy : 56.410256\n",
      "Rejected Accuracy : 16\n",
      "Overall Accuracy  : 0.026000\n",
      "Error             : 10.683761\n",
      "----------Quantitative Results for Threshold----------\n",
      "-------------------Results for Threshold: 0.900000 -------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ---------------------\n",
    "#  Test\n",
    "# ---------------------\n",
    "def test_models():\n",
    "    load_or_train_models()\n",
    "    print(\"Generating images and printing results...\")\n",
    "    for threshold_index in range(len(parameters[\"threshold\"])):\n",
    "        print('-------------------Results for Threshold: %2f -------------------' % parameters[\"threshold\"][threshold_index])\n",
    "        counter = 0\n",
    "        probUnderThreshold = 0\n",
    "        probUnderThresholdButCorrectClassified = 0\n",
    "        probUnderThresholdThenClassifiedCorrectClassified = 0\n",
    "        probAboveThresholdAndCorrectClassified = 0\n",
    "        probAboveThresholdAlsoClassifiedCorrectClassified = 0\n",
    "\n",
    "        for i in range(10):\n",
    "            for j in range(int(len(testset) / parameters[\"batch_size\"])):\n",
    "                # obtain one batch of training images\n",
    "                dataiter = iter(testloader)\n",
    "                images, labels = dataiter.next()\n",
    "\n",
    "                images, labels = arrange_data_tensors(images, labels)\n",
    "\n",
    "                # get sample outputs\n",
    "                output = model(images)\n",
    "\n",
    "                predsProbs, preds = predict(images, labels)\n",
    "\n",
    "                # convert output probabilities to predicted class\n",
    "                # predsProbs, preds = torch.max(output, 1)\n",
    "\n",
    "                for k in range(parameters[\"batch_size\"]):\n",
    "\n",
    "                    idx = (i * int(len(testset))) + (j * parameters[\"batch_size\"]) + k\n",
    "\n",
    "                    trueLabel = labels[k].item()\n",
    "                    modelsPrediction = preds[k].item()\n",
    "                    modelsPredictionProb = predsProbs[k].item()\n",
    "\n",
    "                    if (modelsPredictionProb < parameters[\"threshold\"][threshold_index]):\n",
    "                        probUnderThreshold = probUnderThreshold + 1\n",
    "                        if (trueLabel == modelsPrediction):\n",
    "                            probUnderThresholdButCorrectClassified = probUnderThresholdButCorrectClassified + 1\n",
    "                    else:\n",
    "                        if (trueLabel == modelsPrediction):\n",
    "                            probAboveThresholdAndCorrectClassified = probAboveThresholdAndCorrectClassified + 1\n",
    "\n",
    "                    if (predsProbs[k] < parameters[\"threshold\"][threshold_index]):\n",
    "                        save_image(images[k], \"images/result_images/%d\" % idx + \"_original_image.png\", nrow=1,\n",
    "                                   normalize=True)\n",
    "\n",
    "                        generatedImage = sample_single_image(images, k,\n",
    "                                                             \"images/result_images/%d\" % idx + \"_generated_image_.png\",\n",
    "                                                             True)\n",
    "                        lastPredsProbs, lastPreds = predict(generatedImage.view(1, 1, 28, 28), labels)\n",
    "\n",
    "                        if not modelsPrediction == trueLabel and lastPreds.item() == trueLabel and counter < 1:\n",
    "                            print('----------Qualitative Results for Threshold----------')\n",
    "                            print('Original Image : %2f (Prediction Acc) || Old Label: %2d' % (predsProbs[k], preds[k]))\n",
    "                            show_image(images[k])\n",
    "                            print(\"Generated Image: %2f (Prediction Acc) || New Label: %2d\" % (lastPredsProbs.item(), lastPreds.item()))\n",
    "                            show_image(generatedImage)\n",
    "                            counter = counter + 1\n",
    "                            print('----------Qualitative Results for Threshold----------\\n')\n",
    "\n",
    "                        if (lastPreds.item() == trueLabel):\n",
    "                            probUnderThresholdThenClassifiedCorrectClassified = probUnderThresholdThenClassifiedCorrectClassified + 1\n",
    "\n",
    "        total_images = 100000.0\n",
    "\n",
    "        reject = (float(100 * probUnderThreshold) / total_images)\n",
    "        bnnAcc = (100 * probUnderThresholdButCorrectClassified / probUnderThreshold)\n",
    "        bnn_and_gwinAcc = (100 * probUnderThresholdThenClassifiedCorrectClassified / probUnderThreshold)\n",
    "        overallAcc = (float(100 * (\n",
    "                probUnderThresholdThenClassifiedCorrectClassified - probUnderThresholdButCorrectClassified)) / total_images)\n",
    "        error = (100 * (bnn_and_gwinAcc - bnnAcc)) / probUnderThreshold\n",
    "\n",
    "        print('----------Quantitative Results for Threshold----------')\n",
    "        print('Reject            : %2f' % reject)\n",
    "        print('BNN Accuracy      : %2f' % bnnAcc)\n",
    "        print('BNN+GWIN Accuracy : %2f' % bnn_and_gwinAcc)\n",
    "        print('Rejected Accuracy : %2d' % (bnn_and_gwinAcc - bnnAcc))\n",
    "        print('Overall Accuracy  : %2f' % overallAcc)\n",
    "        print('Error             : %2f' % error)\n",
    "        print('----------Quantitative Results for Threshold----------')\n",
    "\n",
    "        print('-------------------Results for Threshold: %2f -------------------\\n\\n' % parameters[\"threshold\"][threshold_index])\n",
    "\n",
    "        \n",
    "test_models()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
